{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69cb9bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luhung3080/miniconda3/envs/chou/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch ver .  1.11.0+cu113\n",
      "Is CUDA available? True\n",
      "pynio ver .  1.5.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "print(\"pytorch ver . \",torch.__version__)\n",
    "print(\"Is CUDA available?\",torch.cuda.is_available())\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.utils.data as Data\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import xarray as xr\n",
    "import os\n",
    "os.environ['R_HOME'] = '/home/luhung3080/miniconda3/envs/chou/lib/R'\n",
    "from rpy2.robjects import r, numpy2ri\n",
    "numpy2ri.activate()\n",
    "from rpy2.robjects.packages import importr\n",
    "sinkr = importr('sinkr')\n",
    "import Nio\n",
    "print (\"pynio ver . \",Nio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8610dcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>date</th>\n",
       "      <th>FCST_TIME</th>\n",
       "      <th>TAU</th>\n",
       "      <th>pm25_cal</th>\n",
       "      <th>pm25_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4.9510</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 10:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>4.4674</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 11:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.6159</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 12:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>3.9937</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 13:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>3.9602</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092755</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 04:00:00</td>\n",
       "      <td>68</td>\n",
       "      <td>3.6190</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092756</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 05:00:00</td>\n",
       "      <td>69</td>\n",
       "      <td>3.7908</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092757</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 06:00:00</td>\n",
       "      <td>70</td>\n",
       "      <td>4.0454</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092758</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 07:00:00</td>\n",
       "      <td>71</td>\n",
       "      <td>3.9015</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092759</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 08:00:00</td>\n",
       "      <td>72</td>\n",
       "      <td>2.7468</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3092760 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SITE_ID                 date            FCST_TIME  TAU  pm25_cal  \\\n",
       "0        EPA001  2020-02-24 08:00:00  2020-02-24 09:00:00    1    4.9510   \n",
       "1        EPA001  2020-02-24 08:00:00  2020-02-24 10:00:00    2    4.4674   \n",
       "2        EPA001  2020-02-24 08:00:00  2020-02-24 11:00:00    3    4.6159   \n",
       "3        EPA001  2020-02-24 08:00:00  2020-02-24 12:00:00    4    3.9937   \n",
       "4        EPA001  2020-02-24 08:00:00  2020-02-24 13:00:00    5    3.9602   \n",
       "...         ...                  ...                  ...  ...       ...   \n",
       "3092755  EPA080  2021-10-30 08:00:00  2021-11-02 04:00:00   68    3.6190   \n",
       "3092756  EPA080  2021-10-30 08:00:00  2021-11-02 05:00:00   69    3.7908   \n",
       "3092757  EPA080  2021-10-30 08:00:00  2021-11-02 06:00:00   70    4.0454   \n",
       "3092758  EPA080  2021-10-30 08:00:00  2021-11-02 07:00:00   71    3.9015   \n",
       "3092759  EPA080  2021-10-30 08:00:00  2021-11-02 08:00:00   72    2.7468   \n",
       "\n",
       "         pm25_obs  \n",
       "0            10.0  \n",
       "1            13.0  \n",
       "2            11.0  \n",
       "3            11.0  \n",
       "4             9.0  \n",
       "...           ...  \n",
       "3092755       4.0  \n",
       "3092756       7.0  \n",
       "3092757       7.0  \n",
       "3092758       4.0  \n",
       "3092759       4.0  \n",
       "\n",
       "[3092760 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('/home/luhung3080/Desktop/PycharmProjects/NCHUproject/Transformer/data_final.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c9ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604, 1704)\n",
      "(604, 5112)\n"
     ]
    }
   ],
   "source": [
    "x1=np.zeros([604,1704])\n",
    "x2=np.zeros([604,5112])\n",
    "\n",
    "#x1 (0th~1703th column as x)\n",
    "for i in range (0,604):\n",
    "    for j in range (0,71):\n",
    "        a=np.array(data['pm25_obs'][5112*i+72*j:5112*i+72*j+24])\n",
    "        for k in range (0,24):\n",
    "            x1[i][j*24+k]=a[k]\n",
    "\n",
    "#x2 (1704th~8519th column as x)\n",
    "for i in range (1,604):\n",
    "    b=np.array(data['pm25_cal'][5112*i:5112*i+5112])\n",
    "    for j in range(0,5112):\n",
    "        x2[i-1][j]=b[j]\n",
    "        \n",
    "print(np.shape(x1))\n",
    "print(np.shape(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c68d1d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604, 1704)\n",
      "(604, 5112)\n"
     ]
    }
   ],
   "source": [
    "x1Restruct_Fun=x1\n",
    "x2Restruct_Fun=x2\n",
    "print(np.shape(x1Restruct_Fun))\n",
    "print(np.shape(x2Restruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2747dd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(601, 5112)\n"
     ]
    }
   ],
   "source": [
    "YRestruct_Fun=np.zeros([601,5112])\n",
    "for j in range (0,71):\n",
    "    for i in range(0,601):\n",
    "        YRestruct_Fun[i][72*j:72*j+24]=x1Restruct_Fun[1+i][24*j:24*j+24]\n",
    "        YRestruct_Fun[i][72*j+24:72*j+48]=x1Restruct_Fun[1+i+1][24*j:24*j+24]\n",
    "        YRestruct_Fun[i][72*j+48:72*j+72]=x1Restruct_Fun[1+i+2][24*j:24*j+24]\n",
    "print(np.shape(YRestruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a055d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(601, 6816)\n"
     ]
    }
   ],
   "source": [
    "XRestruct_Fun=np.zeros([601,6816])\n",
    "for i in range (0,601):\n",
    "    for j in range (0,1704):\n",
    "        XRestruct_Fun[i][j]=x1Restruct_Fun[i][j]\n",
    "    for j in range (1704,6816):\n",
    "        XRestruct_Fun[i][j]=x2Restruct_Fun[i][j-1704]\n",
    "print(np.shape(XRestruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9ef344",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=XRestruct_Fun\n",
    "Yhat=YRestruct_Fun\n",
    "X1hat_train = np.zeros([540,1704])\n",
    "X2hat_train = np.zeros([540,5112])\n",
    "Yhat_train = np.zeros([540,5112])\n",
    "X1hat_test = np.zeros([61,1704])\n",
    "X2hat_test = np.zeros([61,5112])\n",
    "Yhat_test = np.zeros([61,5112])\n",
    "for i in range (0,540):\n",
    "    for j in range (0,1704):\n",
    "        X1hat_train[i][j] = Xhat[i][j]\n",
    "    for j in range (1704,6816):\n",
    "        X2hat_train[i][j-1704] = Xhat[i][j]\n",
    "    for j in range (0,5112):\n",
    "        Yhat_train[i][j] = Yhat[i][j]\n",
    "for i in range (540,601):\n",
    "    for j in range (0,1704):\n",
    "        X1hat_test[i-540][j] = Xhat[i][j]    \n",
    "    for j in range (1704,6816):\n",
    "        X2hat_test[i-540][j-1704] = Xhat[i][j]\n",
    "    for j in range (0,5112):     \n",
    "        Yhat_test[i-540][j] = Yhat[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d23224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1hat_train\n",
      "(540, 1704)\n",
      "X2hat_train\n",
      "(540, 5112)\n",
      "Yhat_train\n",
      "(540, 5112)\n",
      "X1hat_test\n",
      "(61, 1704)\n",
      "X2hat_test\n",
      "(61, 5112)\n",
      "Yhat_test\n",
      "(61, 5112)\n"
     ]
    }
   ],
   "source": [
    "print('X1hat_train')\n",
    "#print(X1hat_train)\n",
    "print(np.shape(X1hat_train))\n",
    "print('X2hat_train')\n",
    "#print(X2hat_train)\n",
    "print(np.shape(X2hat_train))\n",
    "print('Yhat_train')\n",
    "#print(Yhat_train)\n",
    "print(np.shape(Yhat_train))\n",
    "print('X1hat_test')\n",
    "#print(X1hat_test)\n",
    "print(np.shape(X1hat_test))\n",
    "print('X2hat_test')\n",
    "#print(X2hat_test)\n",
    "print(np.shape(X2hat_test))\n",
    "print('Yhat_test')\n",
    "#print(Yhat_test)\n",
    "print(np.shape(Yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84809cc4",
   "metadata": {},
   "source": [
    "# GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d700356",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X1hat_train\n",
    "x2 = X2hat_train\n",
    "y = Yhat_train\n",
    "xt1 = X1hat_test\n",
    "xt2 = X2hat_test\n",
    "yt = Yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "733cd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (x1,x2,b0,b1,b2):\n",
    "    y = b0 +  torch.matmul(x1,b1) + torch.matmul(x2,b2)\n",
    "#    y = b0 +  b1*x1 + torch.matmul(x2,b2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89bf7839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 94251.3672, Testing loss 99211.3229\n",
      "Epoch 1, Training loss 94157.1016, Testing loss 99112.0035\n",
      "Epoch 2, Training loss 94062.8438, Testing loss 99012.6836\n",
      "Epoch 3, Training loss 93968.5703, Testing loss 98913.3581\n",
      "Epoch 4, Training loss 93874.3125, Testing loss 98814.0281\n",
      "Epoch 5, Training loss 93780.0391, Testing loss 98714.7157\n",
      "Epoch 6, Training loss 93685.7812, Testing loss 98615.4018\n",
      "Epoch 7, Training loss 93591.5312, Testing loss 98516.0701\n",
      "Epoch 8, Training loss 93497.2500, Testing loss 98416.7487\n",
      "Epoch 9, Training loss 93402.9844, Testing loss 98317.4209\n",
      "Epoch 10, Training loss 93308.7188, Testing loss 98218.1048\n",
      "Epoch 20, Training loss 92366.0625, Testing loss 97224.9121\n",
      "Epoch 30, Training loss 91423.4141, Testing loss 96231.6494\n",
      "Epoch 40, Training loss 90480.7656, Testing loss 95238.4402\n",
      "Epoch 50, Training loss 89538.1016, Testing loss 94245.2122\n",
      "Epoch 60, Training loss 88595.4609, Testing loss 93251.9930\n",
      "Epoch 70, Training loss 87652.8125, Testing loss 92258.7816\n",
      "Epoch 80, Training loss 86710.1484, Testing loss 91265.5516\n",
      "Epoch 90, Training loss 85767.5000, Testing loss 90272.3267\n",
      "Epoch 100, Training loss 84824.8438, Testing loss 89279.1033\n",
      "Epoch 110, Training loss 83882.1953, Testing loss 88285.8787\n",
      "Epoch 120, Training loss 82939.5625, Testing loss 87292.6604\n",
      "Epoch 130, Training loss 81996.8984, Testing loss 86299.4366\n",
      "Epoch 140, Training loss 81054.2422, Testing loss 85306.2196\n",
      "Epoch 150, Training loss 80111.5938, Testing loss 84312.9909\n",
      "Epoch 160, Training loss 79168.9375, Testing loss 83319.7766\n",
      "Epoch 170, Training loss 78226.2891, Testing loss 82326.5502\n",
      "Epoch 180, Training loss 77283.6328, Testing loss 81333.3321\n",
      "Epoch 190, Training loss 76340.9688, Testing loss 80340.1063\n",
      "Epoch 200, Training loss 75398.3281, Testing loss 79346.8876\n",
      "Epoch 210, Training loss 74455.6797, Testing loss 78353.6717\n",
      "Epoch 220, Training loss 73513.0234, Testing loss 77360.4498\n",
      "Epoch 230, Training loss 72570.3750, Testing loss 76367.2197\n",
      "Epoch 240, Training loss 71627.7109, Testing loss 75374.0112\n",
      "Epoch 250, Training loss 70685.0547, Testing loss 74380.7829\n",
      "Epoch 260, Training loss 69742.4219, Testing loss 73387.5525\n",
      "Epoch 270, Training loss 68799.7734, Testing loss 72394.3427\n",
      "Epoch 280, Training loss 67857.1172, Testing loss 71401.1138\n",
      "Epoch 290, Training loss 66914.4609, Testing loss 70407.8920\n",
      "Epoch 300, Training loss 65971.8125, Testing loss 69414.6690\n",
      "Epoch 310, Training loss 65029.1562, Testing loss 68421.4485\n",
      "Epoch 320, Training loss 64086.5000, Testing loss 67428.2224\n",
      "Epoch 330, Training loss 63143.8477, Testing loss 66435.0053\n",
      "Epoch 340, Training loss 62201.2031, Testing loss 65441.7847\n",
      "Epoch 350, Training loss 61258.5430, Testing loss 64448.5599\n",
      "Epoch 360, Training loss 60315.8984, Testing loss 63455.3405\n",
      "Epoch 370, Training loss 59373.2344, Testing loss 62462.1152\n",
      "Epoch 380, Training loss 58430.5938, Testing loss 61468.8972\n",
      "Epoch 390, Training loss 57487.9414, Testing loss 60475.6808\n",
      "Epoch 400, Training loss 56545.2773, Testing loss 59482.4536\n",
      "Epoch 410, Training loss 55602.6289, Testing loss 58489.2298\n",
      "Epoch 420, Training loss 54659.9727, Testing loss 57496.0065\n",
      "Epoch 430, Training loss 53717.3320, Testing loss 56502.7866\n",
      "Epoch 440, Training loss 52774.6680, Testing loss 55509.5552\n",
      "Epoch 450, Training loss 51832.0156, Testing loss 54516.3403\n",
      "Epoch 460, Training loss 50889.3711, Testing loss 53523.1225\n",
      "Epoch 470, Training loss 49946.7188, Testing loss 52529.9003\n",
      "Epoch 480, Training loss 49004.0664, Testing loss 51536.6758\n",
      "Epoch 490, Training loss 48061.4141, Testing loss 50543.4530\n",
      "Epoch 500, Training loss 47118.7656, Testing loss 49550.2317\n",
      "Epoch 510, Training loss 46176.1016, Testing loss 48557.0099\n",
      "Epoch 520, Training loss 45233.4531, Testing loss 47563.7788\n",
      "Epoch 530, Training loss 44290.8008, Testing loss 46570.5655\n",
      "Epoch 540, Training loss 43348.1523, Testing loss 45577.3448\n",
      "Epoch 550, Training loss 42405.4922, Testing loss 44584.1260\n",
      "Epoch 560, Training loss 41462.8438, Testing loss 43590.9022\n",
      "Epoch 570, Training loss 40520.1914, Testing loss 42597.6803\n",
      "Epoch 580, Training loss 39577.5391, Testing loss 41604.4553\n",
      "Epoch 590, Training loss 38634.8867, Testing loss 40611.2369\n",
      "Epoch 600, Training loss 37692.2383, Testing loss 39618.0102\n",
      "Epoch 610, Training loss 36749.5820, Testing loss 38624.7998\n",
      "Epoch 620, Training loss 35806.9336, Testing loss 37631.5734\n",
      "Epoch 630, Training loss 34864.2812, Testing loss 36638.3496\n",
      "Epoch 640, Training loss 33921.6289, Testing loss 35645.1353\n",
      "Epoch 650, Training loss 32978.9766, Testing loss 34651.9061\n",
      "Epoch 660, Training loss 32036.3203, Testing loss 33658.6851\n",
      "Epoch 670, Training loss 31093.6719, Testing loss 32665.4633\n",
      "Epoch 680, Training loss 30151.0156, Testing loss 31672.2402\n",
      "Epoch 690, Training loss 29208.3672, Testing loss 30679.0172\n",
      "Epoch 700, Training loss 28265.7148, Testing loss 29685.7977\n",
      "Epoch 710, Training loss 27323.0605, Testing loss 28692.5789\n",
      "Epoch 720, Training loss 26380.4082, Testing loss 27699.3522\n",
      "Epoch 730, Training loss 25437.7578, Testing loss 26706.1329\n",
      "Epoch 740, Training loss 24495.1094, Testing loss 25712.9095\n",
      "Epoch 750, Training loss 23552.4512, Testing loss 24719.6873\n",
      "Epoch 760, Training loss 22609.7930, Testing loss 23726.4576\n",
      "Epoch 770, Training loss 21667.1387, Testing loss 22733.2304\n",
      "Epoch 780, Training loss 20724.4785, Testing loss 21740.0043\n",
      "Epoch 790, Training loss 19781.8184, Testing loss 20746.7747\n",
      "Epoch 800, Training loss 18839.1602, Testing loss 19753.5463\n",
      "Epoch 810, Training loss 17896.5039, Testing loss 18760.3169\n",
      "Epoch 820, Training loss 16953.8438, Testing loss 17767.0902\n",
      "Epoch 830, Training loss 16011.1875, Testing loss 16773.8621\n",
      "Epoch 840, Training loss 15068.5283, Testing loss 15780.6339\n",
      "Epoch 850, Training loss 14125.8691, Testing loss 14787.4058\n",
      "Epoch 860, Training loss 13183.2139, Testing loss 13794.1777\n",
      "Epoch 870, Training loss 12240.5527, Testing loss 12800.9500\n",
      "Epoch 880, Training loss 11297.8955, Testing loss 11807.7219\n",
      "Epoch 890, Training loss 10355.2383, Testing loss 10814.4933\n",
      "Epoch 900, Training loss 9412.5801, Testing loss 9821.2663\n",
      "Epoch 910, Training loss 8469.9229, Testing loss 8828.0384\n",
      "Epoch 920, Training loss 7527.2637, Testing loss 7834.8116\n",
      "Epoch 930, Training loss 6584.6064, Testing loss 6841.5834\n",
      "Epoch 940, Training loss 5641.9482, Testing loss 5848.3558\n",
      "Epoch 950, Training loss 4699.2900, Testing loss 4855.1276\n",
      "Epoch 960, Training loss 3756.6323, Testing loss 3861.8997\n",
      "Epoch 970, Training loss 2813.9741, Testing loss 2868.6717\n",
      "Epoch 980, Training loss 1871.3162, Testing loss 1875.4437\n",
      "Epoch 990, Training loss 928.6579, Testing loss 882.2158\n",
      "Epoch 1000, Training loss 14.0086, Testing loss 91.3021\n",
      "Epoch 1010, Training loss 177.4473, Testing loss 146.9759\n",
      "Epoch 1020, Training loss 98.3084, Testing loss 87.1531\n",
      "Epoch 1030, Training loss 19.2719, Testing loss 17.6618\n",
      "Epoch 1040, Training loss 23.0043, Testing loss 8.4666\n",
      "Epoch 1050, Training loss 9.7066, Testing loss 14.2362\n",
      "Epoch 1060, Training loss 8.8296, Testing loss 7.0390\n",
      "Epoch 1070, Training loss 5.8946, Testing loss 7.0061\n",
      "Epoch 1080, Training loss 5.2556, Testing loss 5.6471\n",
      "Epoch 1090, Training loss 4.8023, Testing loss 5.2154\n",
      "Epoch 1100, Training loss 4.5145, Testing loss 5.0303\n",
      "Epoch 1110, Training loss 4.2937, Testing loss 4.9766\n",
      "Epoch 1120, Training loss 4.1187, Testing loss 5.0050\n",
      "Epoch 1130, Training loss 3.9957, Testing loss 5.0180\n",
      "Epoch 1140, Training loss 3.8956, Testing loss 5.0500\n",
      "Epoch 1150, Training loss 3.8234, Testing loss 5.0949\n",
      "Epoch 1160, Training loss 3.7711, Testing loss 5.1399\n",
      "Epoch 1170, Training loss 3.7142, Testing loss 5.1861\n",
      "Epoch 1180, Training loss 3.7143, Testing loss 5.2536\n",
      "Epoch 1190, Training loss 3.6931, Testing loss 5.3197\n",
      "Epoch 1200, Training loss 3.7289, Testing loss 5.4257\n",
      "Epoch 1210, Training loss 3.7541, Testing loss 5.4869\n",
      "Epoch 1220, Training loss 3.8222, Testing loss 5.6161\n",
      "Epoch 1230, Training loss 3.8950, Testing loss 5.7232\n",
      "Epoch 1240, Training loss 3.9854, Testing loss 5.8379\n",
      "Epoch 1250, Training loss 4.0543, Testing loss 5.9314\n",
      "Epoch 1260, Training loss 4.1420, Testing loss 6.0605\n",
      "Epoch 1270, Training loss 4.2895, Testing loss 6.1844\n",
      "Epoch 1280, Training loss 4.4010, Testing loss 6.3343\n",
      "Epoch 1290, Training loss 4.5209, Testing loss 6.4707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1300, Training loss 4.6463, Testing loss 6.5533\n",
      "Epoch 1310, Training loss 4.7598, Testing loss 6.7171\n",
      "Epoch 1320, Training loss 4.9274, Testing loss 6.8070\n",
      "Epoch 1330, Training loss 4.9417, Testing loss 6.8947\n",
      "Epoch 1340, Training loss 5.1124, Testing loss 6.9655\n",
      "Epoch 1350, Training loss 5.2051, Testing loss 7.1257\n",
      "Epoch 1360, Training loss 5.2191, Testing loss 7.1722\n",
      "Epoch 1370, Training loss 5.3030, Testing loss 7.2666\n",
      "Epoch 1380, Training loss 5.4309, Testing loss 7.3878\n",
      "Epoch 1390, Training loss 5.3960, Testing loss 7.3938\n",
      "Epoch 1400, Training loss 5.4279, Testing loss 7.3925\n",
      "Epoch 1410, Training loss 5.5086, Testing loss 7.4695\n",
      "Epoch 1420, Training loss 5.5795, Testing loss 7.5880\n",
      "Epoch 1430, Training loss 5.6629, Testing loss 7.5991\n",
      "Epoch 1440, Training loss 5.6940, Testing loss 7.6627\n",
      "Epoch 1450, Training loss 5.6699, Testing loss 7.7053\n",
      "Epoch 1460, Training loss 5.7150, Testing loss 7.7440\n",
      "Epoch 1470, Training loss 5.7346, Testing loss 7.7366\n",
      "Epoch 1480, Training loss 5.8456, Testing loss 7.8183\n",
      "Epoch 1490, Training loss 5.8364, Testing loss 7.8068\n",
      "Epoch 1500, Training loss 5.8907, Testing loss 7.9086\n",
      "Epoch 1510, Training loss 5.8656, Testing loss 7.9421\n",
      "Epoch 1520, Training loss 5.9115, Testing loss 7.9921\n",
      "Epoch 1530, Training loss 5.9486, Testing loss 7.9780\n",
      "Epoch 1540, Training loss 5.9283, Testing loss 8.1370\n",
      "Epoch 1550, Training loss 5.9762, Testing loss 8.0329\n",
      "Epoch 1560, Training loss 6.0427, Testing loss 8.1550\n",
      "Epoch 1570, Training loss 6.0535, Testing loss 8.1909\n",
      "Epoch 1580, Training loss 6.1403, Testing loss 8.2460\n",
      "Epoch 1590, Training loss 6.1667, Testing loss 8.2239\n",
      "Epoch 1600, Training loss 6.1832, Testing loss 8.3991\n",
      "Epoch 1610, Training loss 6.3148, Testing loss 8.3749\n",
      "Epoch 1620, Training loss 6.3968, Testing loss 8.4401\n",
      "Epoch 1630, Training loss 6.3890, Testing loss 8.4733\n",
      "Epoch 1640, Training loss 6.4506, Testing loss 8.4655\n",
      "Epoch 1650, Training loss 6.4744, Testing loss 8.5531\n",
      "Epoch 1660, Training loss 6.5519, Testing loss 8.5664\n",
      "Epoch 1670, Training loss 6.4455, Testing loss 8.5899\n",
      "Epoch 1680, Training loss 6.5948, Testing loss 8.6224\n",
      "Epoch 1690, Training loss 6.5965, Testing loss 8.6940\n",
      "Epoch 1700, Training loss 6.5781, Testing loss 8.6902\n",
      "Epoch 1710, Training loss 6.6906, Testing loss 8.7839\n",
      "Epoch 1720, Training loss 6.7307, Testing loss 8.7951\n",
      "Epoch 1730, Training loss 6.6746, Testing loss 8.7900\n",
      "Epoch 1740, Training loss 6.8274, Testing loss 8.7883\n",
      "Epoch 1750, Training loss 6.7317, Testing loss 8.8460\n",
      "Epoch 1760, Training loss 6.7519, Testing loss 8.8692\n",
      "Epoch 1770, Training loss 6.8454, Testing loss 8.9146\n",
      "Epoch 1780, Training loss 6.8308, Testing loss 8.8960\n",
      "Epoch 1790, Training loss 6.7726, Testing loss 8.9294\n",
      "Epoch 1800, Training loss 6.9232, Testing loss 8.9227\n",
      "Epoch 1810, Training loss 6.8302, Testing loss 8.9509\n",
      "Epoch 1820, Training loss 6.8884, Testing loss 8.9415\n",
      "Epoch 1830, Training loss 6.9466, Testing loss 9.0902\n",
      "Epoch 1840, Training loss 6.9129, Testing loss 9.0212\n",
      "Epoch 1850, Training loss 6.8317, Testing loss 9.0390\n",
      "Epoch 1860, Training loss 6.9763, Testing loss 9.1411\n",
      "Epoch 1870, Training loss 6.9951, Testing loss 9.1240\n",
      "Epoch 1880, Training loss 6.9339, Testing loss 9.0573\n",
      "Epoch 1890, Training loss 7.0429, Testing loss 9.3377\n",
      "Epoch 1900, Training loss 7.1199, Testing loss 9.1998\n",
      "Epoch 1910, Training loss 7.0545, Testing loss 9.1481\n",
      "Epoch 1920, Training loss 7.0805, Testing loss 9.2885\n",
      "Epoch 1930, Training loss 7.0901, Testing loss 9.2720\n",
      "Epoch 1940, Training loss 7.0815, Testing loss 9.1974\n",
      "Epoch 1950, Training loss 7.1305, Testing loss 9.2624\n",
      "Epoch 1960, Training loss 7.1333, Testing loss 9.3373\n",
      "Epoch 1970, Training loss 7.2028, Testing loss 9.2707\n",
      "Epoch 1980, Training loss 7.1600, Testing loss 9.4022\n",
      "Epoch 1990, Training loss 7.2833, Testing loss 9.3249\n",
      "Epoch 1991, Training loss 7.1991, Testing loss 9.3780\n",
      "Epoch 1992, Training loss 7.2340, Testing loss 9.4340\n",
      "Epoch 1993, Training loss 7.2833, Testing loss 9.3966\n",
      "Epoch 1994, Training loss 7.2407, Testing loss 9.3877\n",
      "Epoch 1995, Training loss 7.2341, Testing loss 9.4173\n",
      "Epoch 1996, Training loss 7.2609, Testing loss 9.3781\n",
      "Epoch 1997, Training loss 7.2454, Testing loss 9.4015\n",
      "Epoch 1998, Training loss 7.2669, Testing loss 9.4147\n",
      "Epoch 1999, Training loss 7.2823, Testing loss 9.3511\n",
      "Epoch 2000, Training loss 7.2028, Testing loss 9.4102\n"
     ]
    }
   ],
   "source": [
    "features1 = torch.from_numpy(x1)\n",
    "features2 = torch.from_numpy(x2)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test1 = torch.from_numpy(xt1)\n",
    "x_test2 = torch.from_numpy(xt2)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "\n",
    "beta0 = torch.ones(5112 , requires_grad = True)\n",
    "beta1 = torch.ones([1704,5112], requires_grad = True)\n",
    "#beta1 = torch.ones(5112, requires_grad = True)\n",
    "beta2 = torch.ones([5112,5112], requires_grad = True)\n",
    "\n",
    "rate = 1e-3\n",
    "optimizer = optim.Adam([beta0 , beta1 , beta2], lr=rate)\n",
    "\n",
    "epo = 2001\n",
    "loss = nn.L1Loss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "\n",
    "for epoch in range (epo):\n",
    "    yhats_train = model(features1.float() , features2.float(), beta0 , beta1 , beta2)\n",
    "    train_loss = loss(targets.float() , yhats_train)\n",
    "    train_error[epoch] = train_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward() \n",
    "    optimizer.step()    \n",
    "\n",
    "    yhats_test = model(x_test1.float(), x_test2.float() , beta0, beta1 , beta2) \n",
    "#     for i in range (25):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.mean(r)\n",
    "    # test_loss = loss(y_test , yhats_test)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Testing loss {test_loss.item():.4f}\")\n",
    "        # print('\\tBeta_0 : ' , beta0)\n",
    "        # print('\\tBeta_1 : ' , beta1)\n",
    "        # print('\\tBeta_2 : ' , beta2)       \n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                        f\" Testing loss {test_loss.item():.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)\n",
    "            # print('\\tBeta_2 : ' , beta2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26735ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5afa7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.976625332326482\n"
     ]
    }
   ],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96fab4cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 25784.9043, Testing loss 21701.8924\n",
      "Epoch 1, Training loss 25759.1074, Testing loss 21680.1584\n",
      "Epoch 2, Training loss 25733.3066, Testing loss 21658.4242\n",
      "Epoch 3, Training loss 25707.5078, Testing loss 21636.6838\n",
      "Epoch 4, Training loss 25681.7070, Testing loss 21614.9415\n",
      "Epoch 5, Training loss 25655.9004, Testing loss 21593.2138\n",
      "Epoch 6, Training loss 25630.1035, Testing loss 21571.4877\n",
      "Epoch 7, Training loss 25604.3184, Testing loss 21549.7405\n",
      "Epoch 8, Training loss 25578.5078, Testing loss 21528.0057\n",
      "Epoch 9, Training loss 25552.7090, Testing loss 21506.2626\n",
      "Epoch 10, Training loss 25526.9082, Testing loss 21484.5337\n",
      "Epoch 20, Training loss 25268.9102, Testing loss 21267.2015\n",
      "Epoch 30, Training loss 25010.9141, Testing loss 21049.7978\n",
      "Epoch 40, Training loss 24752.9199, Testing loss 20832.4496\n",
      "Epoch 50, Training loss 24494.9141, Testing loss 20615.0810\n",
      "Epoch 60, Training loss 24236.9258, Testing loss 20397.7238\n",
      "Epoch 70, Training loss 23978.9336, Testing loss 20180.3712\n",
      "Epoch 80, Training loss 23720.9316, Testing loss 19963.0016\n",
      "Epoch 90, Training loss 23462.9355, Testing loss 19745.6376\n",
      "Epoch 100, Training loss 23204.9375, Testing loss 19528.2732\n",
      "Epoch 110, Training loss 22946.9414, Testing loss 19310.9099\n",
      "Epoch 120, Training loss 22688.9590, Testing loss 19093.5508\n",
      "Epoch 130, Training loss 22430.9512, Testing loss 18876.1876\n",
      "Epoch 140, Training loss 22172.9531, Testing loss 18658.8310\n",
      "Epoch 150, Training loss 21914.9570, Testing loss 18441.4621\n",
      "Epoch 160, Training loss 21656.9629, Testing loss 18224.1071\n",
      "Epoch 170, Training loss 21398.9668, Testing loss 18006.7424\n",
      "Epoch 180, Training loss 21140.9707, Testing loss 17789.3842\n",
      "Epoch 190, Training loss 20882.9707, Testing loss 17572.0173\n",
      "Epoch 200, Training loss 20624.9766, Testing loss 17354.6596\n",
      "Epoch 210, Training loss 20366.9785, Testing loss 17137.3031\n",
      "Epoch 220, Training loss 20108.9824, Testing loss 16919.9425\n",
      "Epoch 230, Training loss 19850.9902, Testing loss 16702.5725\n",
      "Epoch 240, Training loss 19592.9883, Testing loss 16485.2239\n",
      "Epoch 250, Training loss 19334.9746, Testing loss 16267.8563\n",
      "Epoch 260, Training loss 19077.0000, Testing loss 16050.4851\n",
      "Epoch 270, Training loss 18819.0039, Testing loss 15833.1365\n",
      "Epoch 280, Training loss 18561.0078, Testing loss 15615.7679\n",
      "Epoch 290, Training loss 18303.0117, Testing loss 15398.4058\n",
      "Epoch 300, Training loss 18045.0098, Testing loss 15181.0436\n",
      "Epoch 310, Training loss 17787.0137, Testing loss 14963.6829\n",
      "Epoch 320, Training loss 17529.0195, Testing loss 14746.3163\n",
      "Epoch 330, Training loss 17271.0156, Testing loss 14528.9600\n",
      "Epoch 340, Training loss 17013.0293, Testing loss 14311.5999\n",
      "Epoch 350, Training loss 16755.0293, Testing loss 14094.2356\n",
      "Epoch 360, Training loss 16497.0352, Testing loss 13876.8755\n",
      "Epoch 370, Training loss 16239.0293, Testing loss 13659.5096\n",
      "Epoch 380, Training loss 15981.0410, Testing loss 13442.1527\n",
      "Epoch 390, Training loss 15723.0459, Testing loss 13224.7970\n",
      "Epoch 400, Training loss 15465.0469, Testing loss 13007.4294\n",
      "Epoch 410, Training loss 15207.0547, Testing loss 12790.0655\n",
      "Epoch 420, Training loss 14949.0527, Testing loss 12572.7031\n",
      "Epoch 430, Training loss 14691.0625, Testing loss 12355.3432\n",
      "Epoch 440, Training loss 14433.0615, Testing loss 12137.9715\n",
      "Epoch 450, Training loss 14175.0654, Testing loss 11920.6174\n",
      "Epoch 460, Training loss 13917.0674, Testing loss 11703.2590\n",
      "Epoch 470, Training loss 13659.0723, Testing loss 11485.8972\n",
      "Epoch 480, Training loss 13401.0752, Testing loss 11268.5333\n",
      "Epoch 490, Training loss 13143.0830, Testing loss 11051.1708\n",
      "Epoch 500, Training loss 12885.0859, Testing loss 10833.8092\n",
      "Epoch 510, Training loss 12627.0869, Testing loss 10616.4483\n",
      "Epoch 520, Training loss 12369.0918, Testing loss 10399.0772\n",
      "Epoch 530, Training loss 12111.0957, Testing loss 10181.7241\n",
      "Epoch 540, Training loss 11853.1025, Testing loss 9964.3634\n",
      "Epoch 550, Training loss 11595.0996, Testing loss 9747.0043\n",
      "Epoch 560, Training loss 11337.1045, Testing loss 9529.6411\n",
      "Epoch 570, Training loss 11079.1094, Testing loss 9312.2795\n",
      "Epoch 580, Training loss 10821.1113, Testing loss 9094.9146\n",
      "Epoch 590, Training loss 10563.1182, Testing loss 8877.5563\n",
      "Epoch 600, Training loss 10305.1191, Testing loss 8660.1903\n",
      "Epoch 610, Training loss 10047.1240, Testing loss 8442.8397\n",
      "Epoch 620, Training loss 9789.1299, Testing loss 8225.4737\n",
      "Epoch 630, Training loss 9531.1299, Testing loss 8008.1096\n",
      "Epoch 640, Training loss 9273.1348, Testing loss 7790.7556\n",
      "Epoch 650, Training loss 9015.1377, Testing loss 7573.3870\n",
      "Epoch 660, Training loss 8757.1406, Testing loss 7356.0256\n",
      "Epoch 670, Training loss 8499.1455, Testing loss 7138.6641\n",
      "Epoch 680, Training loss 8241.1445, Testing loss 6921.3015\n",
      "Epoch 690, Training loss 7983.1514, Testing loss 6703.9385\n",
      "Epoch 700, Training loss 7725.1558, Testing loss 6486.5790\n",
      "Epoch 710, Training loss 7467.1577, Testing loss 6269.2205\n",
      "Epoch 720, Training loss 7209.1611, Testing loss 6051.8541\n",
      "Epoch 730, Training loss 6951.1680, Testing loss 5834.4947\n",
      "Epoch 740, Training loss 6693.1753, Testing loss 5617.1320\n",
      "Epoch 750, Training loss 6435.1733, Testing loss 5399.7701\n",
      "Epoch 760, Training loss 6177.1748, Testing loss 5182.4054\n",
      "Epoch 770, Training loss 5919.1792, Testing loss 4965.0431\n",
      "Epoch 780, Training loss 5661.1787, Testing loss 4747.6821\n",
      "Epoch 790, Training loss 5403.1812, Testing loss 4530.3175\n",
      "Epoch 800, Training loss 5145.1821, Testing loss 4312.9542\n",
      "Epoch 810, Training loss 4887.1846, Testing loss 4095.5899\n",
      "Epoch 820, Training loss 4629.1865, Testing loss 3878.2279\n",
      "Epoch 830, Training loss 4371.1880, Testing loss 3660.8649\n",
      "Epoch 840, Training loss 4113.1899, Testing loss 3443.5016\n",
      "Epoch 850, Training loss 3855.1912, Testing loss 3226.1385\n",
      "Epoch 860, Training loss 3597.1938, Testing loss 3008.7751\n",
      "Epoch 870, Training loss 3339.1948, Testing loss 2791.4126\n",
      "Epoch 880, Training loss 3081.1975, Testing loss 2574.0494\n",
      "Epoch 890, Training loss 2823.1990, Testing loss 2356.6856\n",
      "Epoch 900, Training loss 2565.2012, Testing loss 2139.3231\n",
      "Epoch 910, Training loss 2307.2034, Testing loss 1921.9603\n",
      "Epoch 920, Training loss 2049.2051, Testing loss 1704.5979\n",
      "Epoch 930, Training loss 1791.2069, Testing loss 1487.2342\n",
      "Epoch 940, Training loss 1533.2092, Testing loss 1269.8714\n",
      "Epoch 950, Training loss 1275.2111, Testing loss 1052.5082\n",
      "Epoch 960, Training loss 1017.2128, Testing loss 835.1452\n",
      "Epoch 970, Training loss 759.2148, Testing loss 617.7821\n",
      "Epoch 980, Training loss 501.2167, Testing loss 400.4190\n",
      "Epoch 990, Training loss 243.2186, Testing loss 183.0560\n",
      "Epoch 1000, Training loss 14.1128, Testing loss 28.9276\n",
      "Epoch 1010, Training loss 54.9713, Testing loss 38.1616\n",
      "Epoch 1020, Training loss 30.5877, Testing loss 22.6049\n",
      "Epoch 1030, Training loss 12.7064, Testing loss 6.8391\n",
      "Epoch 1040, Training loss 7.8248, Testing loss 7.7533\n",
      "Epoch 1050, Training loss 6.4841, Testing loss 5.7936\n",
      "Epoch 1060, Training loss 6.0949, Testing loss 5.5716\n",
      "Epoch 1070, Training loss 5.8391, Testing loss 5.4795\n",
      "Epoch 1080, Training loss 5.6899, Testing loss 5.4437\n",
      "Epoch 1090, Training loss 5.5888, Testing loss 5.4227\n",
      "Epoch 1100, Training loss 5.5079, Testing loss 5.4099\n",
      "Epoch 1110, Training loss 5.4369, Testing loss 5.4012\n",
      "Epoch 1120, Training loss 5.3719, Testing loss 5.3935\n",
      "Epoch 1130, Training loss 5.3114, Testing loss 5.3869\n",
      "Epoch 1140, Training loss 5.2543, Testing loss 5.3801\n",
      "Epoch 1150, Training loss 5.2000, Testing loss 5.3735\n",
      "Epoch 1160, Training loss 5.1481, Testing loss 5.3678\n",
      "Epoch 1170, Training loss 5.0982, Testing loss 5.3619\n",
      "Epoch 1180, Training loss 5.0500, Testing loss 5.3574\n",
      "Epoch 1190, Training loss 5.0035, Testing loss 5.3524\n",
      "Epoch 1200, Training loss 4.9582, Testing loss 5.3481\n",
      "Epoch 1210, Training loss 4.9142, Testing loss 5.3441\n",
      "Epoch 1220, Training loss 4.8712, Testing loss 5.3402\n",
      "Epoch 1230, Training loss 4.8295, Testing loss 5.3374\n",
      "Epoch 1240, Training loss 4.7887, Testing loss 5.3341\n",
      "Epoch 1250, Training loss 4.7485, Testing loss 5.3309\n",
      "Epoch 1260, Training loss 4.7097, Testing loss 5.3286\n",
      "Epoch 1270, Training loss 4.6714, Testing loss 5.3260\n",
      "Epoch 1280, Training loss 4.6339, Testing loss 5.3241\n",
      "Epoch 1290, Training loss 4.5970, Testing loss 5.3217\n",
      "Epoch 1300, Training loss 4.5611, Testing loss 5.3203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1310, Training loss 4.5261, Testing loss 5.3174\n",
      "Epoch 1320, Training loss 4.4909, Testing loss 5.3167\n",
      "Epoch 1330, Training loss 4.4571, Testing loss 5.3148\n",
      "Epoch 1340, Training loss 4.4237, Testing loss 5.3128\n",
      "Epoch 1350, Training loss 4.3912, Testing loss 5.3141\n",
      "Epoch 1360, Training loss 4.3585, Testing loss 5.3107\n",
      "Epoch 1370, Training loss 4.3267, Testing loss 5.3116\n",
      "Epoch 1380, Training loss 4.2958, Testing loss 5.3090\n",
      "Epoch 1390, Training loss 4.2647, Testing loss 5.3096\n",
      "Epoch 1400, Training loss 4.2351, Testing loss 5.3083\n",
      "Epoch 1410, Training loss 4.2050, Testing loss 5.3087\n",
      "Epoch 1420, Training loss 4.1767, Testing loss 5.3073\n",
      "Epoch 1430, Training loss 4.1483, Testing loss 5.3081\n",
      "Epoch 1440, Training loss 4.1208, Testing loss 5.3077\n",
      "Epoch 1450, Training loss 4.0926, Testing loss 5.3082\n",
      "Epoch 1460, Training loss 4.0639, Testing loss 5.3092\n",
      "Epoch 1470, Training loss 4.0377, Testing loss 5.3086\n",
      "Epoch 1480, Training loss 4.0111, Testing loss 5.3083\n",
      "Epoch 1490, Training loss 3.9846, Testing loss 5.3099\n",
      "Epoch 1500, Training loss 3.9604, Testing loss 5.3111\n",
      "Epoch 1510, Training loss 3.9346, Testing loss 5.3118\n",
      "Epoch 1520, Training loss 3.9107, Testing loss 5.3116\n",
      "Epoch 1530, Training loss 3.8877, Testing loss 5.3136\n",
      "Epoch 1540, Training loss 3.8615, Testing loss 5.3112\n",
      "Epoch 1550, Training loss 3.8372, Testing loss 5.3159\n",
      "Epoch 1560, Training loss 3.8153, Testing loss 5.3171\n",
      "Epoch 1570, Training loss 3.7930, Testing loss 5.3184\n",
      "Epoch 1580, Training loss 3.7696, Testing loss 5.3170\n",
      "Epoch 1590, Training loss 3.7495, Testing loss 5.3197\n",
      "Epoch 1600, Training loss 3.7263, Testing loss 5.3197\n",
      "Epoch 1610, Training loss 3.7067, Testing loss 5.3228\n",
      "Epoch 1620, Training loss 3.6838, Testing loss 5.3230\n",
      "Epoch 1630, Training loss 3.6651, Testing loss 5.3262\n",
      "Epoch 1640, Training loss 3.6456, Testing loss 5.3287\n",
      "Epoch 1650, Training loss 3.6262, Testing loss 5.3304\n",
      "Epoch 1660, Training loss 3.6098, Testing loss 5.3348\n",
      "Epoch 1670, Training loss 3.5897, Testing loss 5.3355\n",
      "Epoch 1680, Training loss 3.5693, Testing loss 5.3391\n",
      "Epoch 1690, Training loss 3.5539, Testing loss 5.3414\n",
      "Epoch 1700, Training loss 3.5388, Testing loss 5.3415\n",
      "Epoch 1710, Training loss 3.5182, Testing loss 5.3479\n",
      "Epoch 1720, Training loss 3.4994, Testing loss 5.3480\n",
      "Epoch 1730, Training loss 3.4859, Testing loss 5.3540\n",
      "Epoch 1740, Training loss 3.4692, Testing loss 5.3541\n",
      "Epoch 1750, Training loss 3.4514, Testing loss 5.3609\n",
      "Epoch 1760, Training loss 3.4379, Testing loss 5.3569\n",
      "Epoch 1770, Training loss 3.4209, Testing loss 5.3658\n",
      "Epoch 1780, Training loss 3.4028, Testing loss 5.3682\n",
      "Epoch 1790, Training loss 3.3964, Testing loss 5.3696\n",
      "Epoch 1800, Training loss 3.3768, Testing loss 5.3750\n",
      "Epoch 1810, Training loss 3.3663, Testing loss 5.3792\n",
      "Epoch 1820, Training loss 3.3558, Testing loss 5.3835\n",
      "Epoch 1830, Training loss 3.3423, Testing loss 5.3886\n",
      "Epoch 1840, Training loss 3.3271, Testing loss 5.3957\n",
      "Epoch 1850, Training loss 3.3164, Testing loss 5.3941\n",
      "Epoch 1860, Training loss 3.2966, Testing loss 5.3964\n",
      "Epoch 1870, Training loss 3.2891, Testing loss 5.4016\n",
      "Epoch 1880, Training loss 3.2830, Testing loss 5.4059\n",
      "Epoch 1890, Training loss 3.2633, Testing loss 5.4099\n",
      "Epoch 1900, Training loss 3.2536, Testing loss 5.4076\n",
      "Epoch 1910, Training loss 3.2448, Testing loss 5.4162\n",
      "Epoch 1920, Training loss 3.2279, Testing loss 5.4195\n",
      "Epoch 1930, Training loss 3.2189, Testing loss 5.4334\n",
      "Epoch 1940, Training loss 3.2155, Testing loss 5.4219\n",
      "Epoch 1950, Training loss 3.1964, Testing loss 5.4427\n",
      "Epoch 1960, Training loss 3.1888, Testing loss 5.4351\n",
      "Epoch 1970, Training loss 3.1845, Testing loss 5.4436\n",
      "Epoch 1980, Training loss 3.1716, Testing loss 5.4459\n",
      "Epoch 1990, Training loss 3.1731, Testing loss 5.4579\n",
      "Epoch 1991, Training loss 3.1593, Testing loss 5.4564\n",
      "Epoch 1992, Training loss 3.1582, Testing loss 5.4579\n",
      "Epoch 1993, Training loss 3.1663, Testing loss 5.4581\n",
      "Epoch 1994, Training loss 3.1605, Testing loss 5.4641\n",
      "Epoch 1995, Training loss 3.1554, Testing loss 5.4555\n",
      "Epoch 1996, Training loss 3.1599, Testing loss 5.4566\n",
      "Epoch 1997, Training loss 3.1522, Testing loss 5.4600\n",
      "Epoch 1998, Training loss 3.1515, Testing loss 5.4626\n",
      "Epoch 1999, Training loss 3.1502, Testing loss 5.4578\n",
      "Epoch 2000, Training loss 3.1513, Testing loss 5.4552\n"
     ]
    }
   ],
   "source": [
    "def model (x1,x2,b0,b1,b2):\n",
    "    y = b0 +  torch.matmul(x1,b1) + b2*x2\n",
    "    return y\n",
    "features1 = torch.from_numpy(x1)\n",
    "features2 = torch.from_numpy(x2)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test1 = torch.from_numpy(xt1)\n",
    "x_test2 = torch.from_numpy(xt2)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "\n",
    "beta0 = torch.ones(5112 , requires_grad = True)\n",
    "beta1 = torch.ones([1704,5112], requires_grad = True)\n",
    "#beta1 = torch.ones(5112, requires_grad = True)\n",
    "beta2 = torch.ones(5112, requires_grad = True)\n",
    "\n",
    "rate = 1e-3\n",
    "optimizer = optim.Adam([beta0 , beta1 , beta2], lr=rate)\n",
    "\n",
    "epo = 2001\n",
    "loss = nn.L1Loss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "\n",
    "for epoch in range (epo):\n",
    "    yhats_train = model(features1.float() , features2.float(), beta0 , beta1 , beta2)\n",
    "    train_loss = loss(targets.float() , yhats_train)\n",
    "    train_error[epoch] = train_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward() \n",
    "    optimizer.step()    \n",
    "\n",
    "    yhats_test = model(x_test1.float(), x_test2.float() , beta0, beta1 , beta2) \n",
    "#     for i in range (25):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.mean(r)\n",
    "    # test_loss = loss(y_test , yhats_test)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Testing loss {test_loss.item():.4f}\")\n",
    "        # print('\\tBeta_0 : ' , beta0)\n",
    "        # print('\\tBeta_1 : ' , beta1)\n",
    "        # print('\\tBeta_2 : ' , beta2)       \n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                        f\" Testing loss {test_loss.item():.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)\n",
    "            # print('\\tBeta_2 : ' , beta2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3a728f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.305869442905403\n"
     ]
    }
   ],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1a750",
   "metadata": {},
   "source": [
    "# LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X1hat_train\n",
    "x2 = X2hat_train\n",
    "y = Yhat_train\n",
    "xt1 = X1hat_test\n",
    "xt2 = X2hat_test\n",
    "yt = Yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (x1,x2,b0,b1,b2):\n",
    "    y = b0 +  torch.matmul(x1,b1) + torch.matmul(x2,b2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5258a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = torch.from_numpy(x1)\n",
    "features2 = torch.from_numpy(x2)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test1 = torch.from_numpy(xt1)\n",
    "x_test2 = torch.from_numpy(xt2)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "beta0 = torch.ones(5112 , requires_grad = True)\n",
    "beta1 = torch.ones([1704,5112], requires_grad = True)\n",
    "beta2 = torch.ones([5112,5112] , requires_grad = True)\n",
    "\n",
    "# rate = 1\n",
    "rate = 1e-2\n",
    "optimizer = optim.LBFGS([beta0 , beta1 , beta2] , lr=rate)\n",
    "# optimizer = optim.Adam([beta0 , beta1 , beta2], lr=rate)\n",
    "# optimizer = optim.SGD([beta0 , beta1 , beta2], lr=rate)\n",
    "\n",
    "epo = 201\n",
    "loss = nn.MSELoss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "for epoch in range (epo):\n",
    "    # yhats_train = model(features.float() , beta0 , beta1)\n",
    "    # train_loss = loss(targets.float() , yhats_train)\n",
    "    # train_error[epoch] = train_loss\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "    # if epoch == 0 :\n",
    "    #         train_loss.backward(retain_graph=True) \n",
    "    # else :\n",
    "    #     train_loss.backward()\n",
    "    # # train_loss.backward() \n",
    "    # optimizer.step()    \n",
    "\n",
    "    def closure():\n",
    "        yhats_train = model(features1.float() , features2.float(), beta0 , beta1 , beta2)\n",
    "        train_loss = loss(targets.float() , yhats_train)\n",
    "        train_error[epoch] = train_loss\n",
    "        optimizer.zero_grad()\n",
    "        # if epoch == epo-1 :\n",
    "        #     train_loss.backward() \n",
    "        # else :\n",
    "        #     train_loss.backward(retain_graph=True)\n",
    "        train_loss.backward(retain_graph=True)\n",
    "        return train_loss\n",
    "    optimizer.step(closure)    \n",
    "\n",
    "    yhats_test = model(x_test1.float(), x_test2.float() , beta0, beta1 , beta2) \n",
    "#     for i in range (25):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.mean(r)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                    f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "        # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "        #             f\" Testing loss {test_loss.item():.4f}\")\n",
    "\n",
    "        # print('\\tBeta_0 : ' , beta0)\n",
    "        # print('\\tBeta_1 : ' , beta1)\n",
    "        # print('\\tBeta_2 : ' , beta2)       \n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                        f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "            # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "            #             f\" Testing loss {test_loss.item():.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)\n",
    "            # print('\\tBeta_2 : ' , beta2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daef3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chou",
   "language": "python",
   "name": "chou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
