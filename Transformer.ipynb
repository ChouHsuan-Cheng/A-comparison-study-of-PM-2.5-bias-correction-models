{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518e65f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luhung3080/miniconda3/envs/chou/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch ver .  1.11.0+cu113\n",
      "Is CUDA available? True\n",
      "pynio ver .  1.5.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "print(\"pytorch ver . \",torch.__version__)\n",
    "print(\"Is CUDA available?\",torch.cuda.is_available())\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.utils.data as Data\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import xarray as xr\n",
    "import os\n",
    "os.environ['R_HOME'] = '/home/luhung3080/miniconda3/envs/chou/lib/R'\n",
    "from rpy2.robjects import r, numpy2ri\n",
    "numpy2ri.activate()\n",
    "from rpy2.robjects.packages import importr\n",
    "sinkr = importr('sinkr')\n",
    "import Nio\n",
    "print (\"pynio ver . \",Nio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b8a17",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe6181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/home/luhung3080/Desktop/PycharmProjects/Data/newdata/result_dineof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12468228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 5183)\n",
      "(605, 5183)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#cal_PMf\n",
    "###\n",
    "u=np.zeros([605,5183])\n",
    "for i in range (0,605):\n",
    "    a=np.array(data['pm25_cal'][5183*i:5183*i+5183])\n",
    "    for j in range(0,5183):\n",
    "        if a[j]=='NaN' :\n",
    "            a[j]=np.nan\n",
    "    u[i]=a.T\n",
    "print(np.shape(u))\n",
    "###\n",
    "#obs_PMf\n",
    "###\n",
    "v=np.zeros([605,5183])\n",
    "for i in range (0,605):\n",
    "    a=np.array(data['pm25_obs'][5183*i:5183*i+5183])\n",
    "    for j in range(0,5183):\n",
    "        if a[j]=='NaN' :\n",
    "            a[j]=np.nan\n",
    "        if a[j]=='A' :\n",
    "            a[j]=np.nan\n",
    "        if a[j]=='*' :\n",
    "            a[j]=np.nan\n",
    "        if a[j]=='x' :\n",
    "            a[j]=np.nan  \n",
    "    v[i]=a.T\n",
    "    \n",
    "print(np.shape(v)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec25e58",
   "metadata": {},
   "source": [
    "## fill data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ced0a430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"1 EOF ; RMS = 7.7073908\"\n",
      "[1] \"1 EOF ; RMS = 7.70725848\"\n",
      "[1] \"1 EOF ; RMS = 7.70729543\"\n",
      "[1] \"2 EOF ; RMS = 6.78652786\"\n",
      "[1] \"2 EOF ; RMS = 6.78629886\"\n",
      "[1] \"2 EOF ; RMS = 6.78630059\"\n",
      "[1] \"3 EOF ; RMS = 6.45414805\"\n",
      "[1] \"3 EOF ; RMS = 6.45518029\"\n",
      "[1] \"4 EOF ; RMS = 6.21616777\"\n",
      "[1] \"4 EOF ; RMS = 6.21647271\"\n",
      "[1] \"5 EOF ; RMS = 6.00158145\"\n",
      "[1] \"5 EOF ; RMS = 6.00137388\"\n",
      "[1] \"5 EOF ; RMS = 6.00138815\"\n",
      "[1] \"6 EOF ; RMS = 5.83228038\"\n",
      "[1] \"6 EOF ; RMS = 5.83261455\"\n",
      "[1] \"7 EOF ; RMS = 5.71928457\"\n",
      "[1] \"7 EOF ; RMS = 5.72038487\"\n",
      "[1] \"8 EOF ; RMS = 5.59669128\"\n",
      "[1] \"8 EOF ; RMS = 5.59672121\"\n",
      "[1] \"9 EOF ; RMS = 5.48484392\"\n",
      "[1] \"9 EOF ; RMS = 5.48514999\"\n",
      "[1] \"10 EOF ; RMS = 5.37527854\"\n",
      "[1] \"10 EOF ; RMS = 5.37586181\"\n",
      "[1] \"11 EOF ; RMS = 5.27336262\"\n",
      "[1] \"11 EOF ; RMS = 5.2739562\"\n",
      "[1] \"12 EOF ; RMS = 5.19970257\"\n",
      "[1] \"12 EOF ; RMS = 5.20114119\"\n",
      "[1] \"13 EOF ; RMS = 5.12320059\"\n",
      "[1] \"13 EOF ; RMS = 5.12384584\"\n",
      "[1] \"14 EOF ; RMS = 5.04746441\"\n",
      "[1] \"14 EOF ; RMS = 5.04771666\"\n",
      "[1] \"15 EOF ; RMS = 4.99433506\"\n",
      "[1] \"15 EOF ; RMS = 4.99552778\"\n",
      "[1] \"16 EOF ; RMS = 4.92174774\"\n",
      "[1] \"16 EOF ; RMS = 4.92152547\"\n",
      "[1] \"16 EOF ; RMS = 4.92154646\"\n",
      "[1] \"17 EOF ; RMS = 4.86025901\"\n",
      "[1] \"17 EOF ; RMS = 4.86102814\"\n",
      "[1] \"18 EOF ; RMS = 4.81834033\"\n",
      "[1] \"18 EOF ; RMS = 4.82074411\"\n",
      "[1] \"19 EOF ; RMS = 4.77756902\"\n",
      "[1] \"19 EOF ; RMS = 4.77839015\"\n",
      "[1] \"20 EOF ; RMS = 4.72838201\"\n",
      "[1] \"20 EOF ; RMS = 4.72907427\"\n",
      "[1] \"21 EOF ; RMS = 4.68101789\"\n",
      "[1] \"21 EOF ; RMS = 4.68174556\"\n",
      "[1] \"22 EOF ; RMS = 4.64487867\"\n",
      "[1] \"22 EOF ; RMS = 4.646225\"\n",
      "[1] \"23 EOF ; RMS = 4.596383\"\n",
      "[1] \"23 EOF ; RMS = 4.59571075\"\n",
      "[1] \"23 EOF ; RMS = 4.59567679\"\n",
      "[1] \"23 EOF ; RMS = 4.59567114\"\n",
      "[1] \"24 EOF ; RMS = 4.55108681\"\n",
      "[1] \"24 EOF ; RMS = 4.55073038\"\n",
      "[1] \"24 EOF ; RMS = 4.55070665\"\n",
      "[1] \"24 EOF ; RMS = 4.55070107\"\n",
      "[1] \"25 EOF ; RMS = 4.50895976\"\n",
      "[1] \"25 EOF ; RMS = 4.5092003\"\n",
      "[1] \"26 EOF ; RMS = 4.48290181\"\n",
      "[1] \"26 EOF ; RMS = 4.48444817\"\n",
      "[1] \"27 EOF ; RMS = 4.45643594\"\n",
      "[1] \"27 EOF ; RMS = 4.45816926\"\n",
      "[1] \"28 EOF ; RMS = 4.42946998\"\n",
      "[1] \"28 EOF ; RMS = 4.43077235\"\n",
      "[1] \"29 EOF ; RMS = 4.40360337\"\n",
      "[1] \"29 EOF ; RMS = 4.40447505\"\n",
      "[1] \"30 EOF ; RMS = 4.38439807\"\n",
      "[1] \"30 EOF ; RMS = 4.38622307\"\n",
      "[1] \"31 EOF ; RMS = 4.36189483\"\n",
      "[1] \"31 EOF ; RMS = 4.3626516\"\n",
      "[1] \"32 EOF ; RMS = 4.33314105\"\n",
      "[1] \"32 EOF ; RMS = 4.3339033\"\n",
      "[1] \"33 EOF ; RMS = 4.30480801\"\n",
      "[1] \"33 EOF ; RMS = 4.30543048\"\n",
      "[1] \"34 EOF ; RMS = 4.28441056\"\n",
      "[1] \"34 EOF ; RMS = 4.28575329\"\n",
      "[1] \"35 EOF ; RMS = 4.26877709\"\n",
      "[1] \"35 EOF ; RMS = 4.2702356\"\n",
      "[1] \"36 EOF ; RMS = 4.25322776\"\n",
      "[1] \"36 EOF ; RMS = 4.25468223\"\n",
      "[1] \"37 EOF ; RMS = 4.23873687\"\n",
      "[1] \"37 EOF ; RMS = 4.24064822\"\n",
      "[1] \"38 EOF ; RMS = 4.22015876\"\n",
      "[1] \"38 EOF ; RMS = 4.22122171\"\n",
      "[1] \"39 EOF ; RMS = 4.20642921\"\n",
      "[1] \"39 EOF ; RMS = 4.20887883\"\n",
      "[1] \"40 EOF ; RMS = 4.19113363\"\n",
      "[1] \"40 EOF ; RMS = 4.1918754\"\n",
      "[1] \"41 EOF ; RMS = 4.17291162\"\n",
      "[1] \"41 EOF ; RMS = 4.17382437\"\n",
      "[1] \"42 EOF ; RMS = 4.15740394\"\n",
      "[1] \"42 EOF ; RMS = 4.15859782\"\n",
      "[1] \"43 EOF ; RMS = 4.14377759\"\n",
      "[1] \"43 EOF ; RMS = 4.14548498\"\n",
      "[1] \"44 EOF ; RMS = 4.13833179\"\n",
      "[1] \"44 EOF ; RMS = 4.14079691\"\n",
      "[1] \"45 EOF ; RMS = 4.12252227\"\n",
      "[1] \"45 EOF ; RMS = 4.12425358\"\n",
      "[1] \"46 EOF ; RMS = 4.10538213\"\n",
      "[1] \"46 EOF ; RMS = 4.1068414\"\n",
      "[1] \"47 EOF ; RMS = 4.08884681\"\n",
      "[1] \"47 EOF ; RMS = 4.08995952\"\n",
      "[1] \"48 EOF ; RMS = 4.0720252\"\n",
      "[1] \"48 EOF ; RMS = 4.07282585\"\n",
      "[1] \"49 EOF ; RMS = 4.06365393\"\n",
      "[1] \"49 EOF ; RMS = 4.06628859\"\n",
      "[1] \"50 EOF ; RMS = 4.05620428\"\n",
      "[1] \"50 EOF ; RMS = 4.05858523\"\n",
      "[1] \"51 EOF ; RMS = 4.04541031\"\n",
      "[1] \"51 EOF ; RMS = 4.04712023\"\n",
      "[1] \"52 EOF ; RMS = 4.03439607\"\n",
      "[1] \"52 EOF ; RMS = 4.03561572\"\n",
      "[1] \"53 EOF ; RMS = 4.02054114\"\n",
      "[1] \"53 EOF ; RMS = 4.02064185\"\n",
      "[1] \"54 EOF ; RMS = 4.0103439\"\n",
      "[1] \"54 EOF ; RMS = 4.0116189\"\n",
      "[1] \"55 EOF ; RMS = 4.0003857\"\n",
      "[1] \"55 EOF ; RMS = 4.00149092\"\n",
      "[1] \"56 EOF ; RMS = 3.99097941\"\n",
      "[1] \"56 EOF ; RMS = 3.99311713\"\n",
      "[1] \"57 EOF ; RMS = 3.98350628\"\n",
      "[1] \"57 EOF ; RMS = 3.98556792\"\n",
      "[1] \"58 EOF ; RMS = 3.98045468\"\n",
      "[1] \"58 EOF ; RMS = 3.98332172\"\n",
      "[1] \"59 EOF ; RMS = 3.97529401\"\n",
      "[1] \"59 EOF ; RMS = 3.97751989\"\n",
      "[1] \"60 EOF ; RMS = 3.96735992\"\n",
      "[1] \"60 EOF ; RMS = 3.96859963\"\n",
      "[1] \"61 EOF ; RMS = 3.95892259\"\n",
      "[1] \"61 EOF ; RMS = 3.96086112\"\n",
      "[1] \"62 EOF ; RMS = 3.95109583\"\n",
      "[1] \"62 EOF ; RMS = 3.95245356\"\n",
      "[1] \"63 EOF ; RMS = 3.9411744\"\n",
      "[1] \"63 EOF ; RMS = 3.94222463\"\n",
      "[1] \"64 EOF ; RMS = 3.93550507\"\n",
      "[1] \"64 EOF ; RMS = 3.93775023\"\n",
      "[1] \"65 EOF ; RMS = 3.92489947\"\n",
      "[1] \"65 EOF ; RMS = 3.92561811\"\n",
      "[1] \"66 EOF ; RMS = 3.913009\"\n",
      "[1] \"66 EOF ; RMS = 3.91394948\"\n",
      "[1] \"67 EOF ; RMS = 3.90080164\"\n",
      "[1] \"67 EOF ; RMS = 3.90126552\"\n",
      "[1] \"68 EOF ; RMS = 3.89230122\"\n",
      "[1] \"68 EOF ; RMS = 3.89387224\"\n",
      "[1] \"69 EOF ; RMS = 3.88539649\"\n",
      "[1] \"69 EOF ; RMS = 3.88717432\"\n",
      "[1] \"70 EOF ; RMS = 3.87843493\"\n",
      "[1] \"70 EOF ; RMS = 3.88019534\"\n",
      "[1] \"71 EOF ; RMS = 3.8723164\"\n",
      "[1] \"71 EOF ; RMS = 3.87402492\"\n",
      "[1] \"72 EOF ; RMS = 3.86465951\"\n",
      "[1] \"72 EOF ; RMS = 3.86595422\"\n",
      "[1] \"73 EOF ; RMS = 3.85767398\"\n",
      "[1] \"73 EOF ; RMS = 3.85987253\"\n",
      "[1] \"74 EOF ; RMS = 3.85759784\"\n",
      "[1] \"74 EOF ; RMS = 3.86087091\"\n",
      "[1] \"75 EOF ; RMS = 3.85037131\"\n",
      "[1] \"75 EOF ; RMS = 3.85119723\"\n",
      "[1] \"76 EOF ; RMS = 3.84212993\"\n",
      "[1] \"76 EOF ; RMS = 3.84296496\"\n",
      "[1] \"77 EOF ; RMS = 3.83655582\"\n",
      "[1] \"77 EOF ; RMS = 3.83854409\"\n",
      "[1] \"78 EOF ; RMS = 3.83272271\"\n",
      "[1] \"78 EOF ; RMS = 3.83456434\"\n",
      "[1] \"79 EOF ; RMS = 3.82699328\"\n",
      "[1] \"79 EOF ; RMS = 3.82854307\"\n",
      "[1] \"80 EOF ; RMS = 3.82665747\"\n",
      "[1] \"80 EOF ; RMS = 3.83005176\"\n",
      "[1] \"81 EOF ; RMS = 3.82639624\"\n",
      "[1] \"81 EOF ; RMS = 3.82925099\"\n",
      "[1] \"82 EOF ; RMS = 3.82615989\"\n",
      "[1] \"82 EOF ; RMS = 3.82966282\"\n",
      "[1] \"83 EOF ; RMS = 3.82516832\"\n",
      "[1] \"83 EOF ; RMS = 3.82762043\"\n",
      "[1] \"84 EOF ; RMS = 3.81911163\"\n",
      "[1] \"84 EOF ; RMS = 3.82049676\"\n",
      "[1] \"85 EOF ; RMS = 3.81413807\"\n",
      "[1] \"85 EOF ; RMS = 3.81608496\"\n",
      "[1] \"86 EOF ; RMS = 3.80735816\"\n",
      "[1] \"86 EOF ; RMS = 3.8086795\"\n",
      "[1] \"87 EOF ; RMS = 3.80193241\"\n",
      "[1] \"87 EOF ; RMS = 3.80393658\"\n",
      "[1] \"88 EOF ; RMS = 3.80108834\"\n",
      "[1] \"88 EOF ; RMS = 3.80341832\"\n",
      "[1] \"89 EOF ; RMS = 3.79746549\"\n",
      "[1] \"89 EOF ; RMS = 3.79915768\"\n",
      "[1] \"90 EOF ; RMS = 3.79443445\"\n",
      "[1] \"90 EOF ; RMS = 3.7961354\"\n",
      "[1] \"91 EOF ; RMS = 3.79097537\"\n",
      "[1] \"91 EOF ; RMS = 3.79217265\"\n",
      "[1] \"92 EOF ; RMS = 3.78536795\"\n",
      "[1] \"92 EOF ; RMS = 3.78627721\"\n",
      "[1] \"93 EOF ; RMS = 3.78174924\"\n",
      "[1] \"93 EOF ; RMS = 3.78396824\"\n",
      "[1] \"94 EOF ; RMS = 3.78047111\"\n",
      "[1] \"94 EOF ; RMS = 3.7825475\"\n",
      "[1] \"95 EOF ; RMS = 3.77508108\"\n",
      "[1] \"95 EOF ; RMS = 3.77587432\"\n",
      "[1] \"96 EOF ; RMS = 3.76770912\"\n",
      "[1] \"96 EOF ; RMS = 3.76888592\"\n",
      "[1] \"97 EOF ; RMS = 3.7633189\"\n",
      "[1] \"97 EOF ; RMS = 3.76548025\"\n",
      "[1] \"98 EOF ; RMS = 3.76092867\"\n",
      "[1] \"98 EOF ; RMS = 3.76288787\"\n",
      "[1] \"99 EOF ; RMS = 3.75952136\"\n",
      "[1] \"99 EOF ; RMS = 3.76205167\"\n",
      "[1] \"100 EOF ; RMS = 3.7579137\"\n",
      "[1] \"100 EOF ; RMS = 3.75970234\"\n",
      "[1] \"101 EOF ; RMS = 3.75133568\"\n",
      "[1] \"101 EOF ; RMS = 3.75181154\"\n",
      "[1] \"102 EOF ; RMS = 3.74818011\"\n",
      "[1] \"102 EOF ; RMS = 3.75065584\"\n",
      "[1] \"103 EOF ; RMS = 3.74586193\"\n",
      "[1] \"103 EOF ; RMS = 3.74790717\"\n",
      "[1] \"104 EOF ; RMS = 3.74513013\"\n",
      "[1] \"104 EOF ; RMS = 3.74738376\"\n",
      "[1] \"105 EOF ; RMS = 3.7469773\"\n",
      "[1] \"105 EOF ; RMS = 3.75055226\"\n",
      "[1] \"106 EOF ; RMS = 3.74465564\"\n",
      "[1] \"106 EOF ; RMS = 3.74626217\"\n",
      "[1] \"107 EOF ; RMS = 3.74099166\"\n",
      "[1] \"107 EOF ; RMS = 3.74294106\"\n",
      "[1] \"108 EOF ; RMS = 3.74151261\"\n",
      "[1] \"108 EOF ; RMS = 3.7446115\"\n",
      "[1] \"109 EOF ; RMS = 3.74141823\"\n",
      "[1] \"109 EOF ; RMS = 3.74421355\"\n",
      "[1] \"110 EOF ; RMS = 3.74324089\"\n",
      "[1] \"110 EOF ; RMS = 3.74624902\"\n",
      "[1] \"111 EOF ; RMS = 3.74235936\"\n",
      "[1] \"111 EOF ; RMS = 3.74444289\"\n",
      "[1] \"112 EOF ; RMS = 3.74494393\"\n",
      "[1] \"1 EOF ; RMS = 7.98921285\"\n",
      "[1] \"1 EOF ; RMS = 7.96184161\"\n",
      "[1] \"1 EOF ; RMS = 7.96126885\"\n",
      "[1] \"1 EOF ; RMS = 7.96119692\"\n",
      "[1] \"1 EOF ; RMS = 7.96118093\"\n",
      "[1] \"1 EOF ; RMS = 7.96117697\"\n",
      "[1] \"2 EOF ; RMS = 7.24680645\"\n",
      "[1] \"2 EOF ; RMS = 7.24494376\"\n",
      "[1] \"2 EOF ; RMS = 7.24483562\"\n",
      "[1] \"2 EOF ; RMS = 7.24481635\"\n",
      "[1] \"2 EOF ; RMS = 7.24481195\"\n",
      "[1] \"3 EOF ; RMS = 6.81733065\"\n",
      "[1] \"3 EOF ; RMS = 6.81544391\"\n",
      "[1] \"3 EOF ; RMS = 6.81525413\"\n",
      "[1] \"3 EOF ; RMS = 6.81521348\"\n",
      "[1] \"3 EOF ; RMS = 6.81520355\"\n",
      "[1] \"4 EOF ; RMS = 6.57632546\"\n",
      "[1] \"4 EOF ; RMS = 6.57618729\"\n",
      "[1] \"4 EOF ; RMS = 6.57616726\"\n",
      "[1] \"4 EOF ; RMS = 6.57614925\"\n",
      "[1] \"4 EOF ; RMS = 6.57614269\"\n",
      "[1] \"5 EOF ; RMS = 6.36047801\"\n",
      "[1] \"5 EOF ; RMS = 6.36153891\"\n",
      "[1] \"6 EOF ; RMS = 6.15691612\"\n",
      "[1] \"6 EOF ; RMS = 6.15790212\"\n",
      "[1] \"7 EOF ; RMS = 6.01420999\"\n",
      "[1] \"7 EOF ; RMS = 6.01431398\"\n",
      "[1] \"8 EOF ; RMS = 5.90268815\"\n",
      "[1] \"8 EOF ; RMS = 5.90404477\"\n",
      "[1] \"9 EOF ; RMS = 5.8158018\"\n",
      "[1] \"9 EOF ; RMS = 5.8163712\"\n",
      "[1] \"10 EOF ; RMS = 5.73213269\"\n",
      "[1] \"10 EOF ; RMS = 5.73361136\"\n",
      "[1] \"11 EOF ; RMS = 5.6525976\"\n",
      "[1] \"11 EOF ; RMS = 5.65285077\"\n",
      "[1] \"12 EOF ; RMS = 5.57609393\"\n",
      "[1] \"12 EOF ; RMS = 5.5765177\"\n",
      "[1] \"13 EOF ; RMS = 5.50554541\"\n",
      "[1] \"13 EOF ; RMS = 5.50576028\"\n",
      "[1] \"14 EOF ; RMS = 5.46313661\"\n",
      "[1] \"14 EOF ; RMS = 5.4648134\"\n",
      "[1] \"15 EOF ; RMS = 5.4216652\"\n",
      "[1] \"15 EOF ; RMS = 5.42302791\"\n",
      "[1] \"16 EOF ; RMS = 5.3741627\"\n",
      "[1] \"16 EOF ; RMS = 5.37490134\"\n",
      "[1] \"17 EOF ; RMS = 5.3320477\"\n",
      "[1] \"17 EOF ; RMS = 5.33291099\"\n",
      "[1] \"18 EOF ; RMS = 5.29661103\"\n",
      "[1] \"18 EOF ; RMS = 5.29773166\"\n",
      "[1] \"19 EOF ; RMS = 5.26297048\"\n",
      "[1] \"19 EOF ; RMS = 5.26450521\"\n",
      "[1] \"20 EOF ; RMS = 5.2222956\"\n",
      "[1] \"20 EOF ; RMS = 5.22196378\"\n",
      "[1] \"20 EOF ; RMS = 5.22198765\"\n",
      "[1] \"21 EOF ; RMS = 5.18382719\"\n",
      "[1] \"21 EOF ; RMS = 5.18354422\"\n",
      "[1] \"21 EOF ; RMS = 5.18360341\"\n",
      "[1] \"22 EOF ; RMS = 5.1522009\"\n",
      "[1] \"22 EOF ; RMS = 5.15258527\"\n",
      "[1] \"23 EOF ; RMS = 5.12845922\"\n",
      "[1] \"23 EOF ; RMS = 5.13052831\"\n",
      "[1] \"24 EOF ; RMS = 5.10540212\"\n",
      "[1] \"24 EOF ; RMS = 5.10685797\"\n",
      "[1] \"25 EOF ; RMS = 5.07670289\"\n",
      "[1] \"25 EOF ; RMS = 5.07672532\"\n",
      "[1] \"26 EOF ; RMS = 5.05229573\"\n",
      "[1] \"26 EOF ; RMS = 5.05419928\"\n",
      "[1] \"27 EOF ; RMS = 5.03246989\"\n",
      "[1] \"27 EOF ; RMS = 5.03490959\"\n",
      "[1] \"28 EOF ; RMS = 5.01748911\"\n",
      "[1] \"28 EOF ; RMS = 5.02008008\"\n",
      "[1] \"29 EOF ; RMS = 5.00531965\"\n",
      "[1] \"29 EOF ; RMS = 5.00767676\"\n",
      "[1] \"30 EOF ; RMS = 4.99786344\"\n",
      "[1] \"30 EOF ; RMS = 5.00218115\"\n",
      "[1] \"31 EOF ; RMS = 4.98516697\"\n",
      "[1] \"31 EOF ; RMS = 4.98766221\"\n",
      "[1] \"32 EOF ; RMS = 4.96110187\"\n",
      "[1] \"32 EOF ; RMS = 4.96179443\"\n",
      "[1] \"33 EOF ; RMS = 4.94089264\"\n",
      "[1] \"33 EOF ; RMS = 4.94162256\"\n",
      "[1] \"34 EOF ; RMS = 4.92541017\"\n",
      "[1] \"34 EOF ; RMS = 4.92735599\"\n",
      "[1] \"35 EOF ; RMS = 4.90865707\"\n",
      "[1] \"35 EOF ; RMS = 4.91095756\"\n",
      "[1] \"36 EOF ; RMS = 4.89021232\"\n",
      "[1] \"36 EOF ; RMS = 4.89241375\"\n",
      "[1] \"37 EOF ; RMS = 4.87832825\"\n",
      "[1] \"37 EOF ; RMS = 4.88150026\"\n",
      "[1] \"38 EOF ; RMS = 4.8676809\"\n",
      "[1] \"38 EOF ; RMS = 4.86991565\"\n",
      "[1] \"39 EOF ; RMS = 4.85058089\"\n",
      "[1] \"39 EOF ; RMS = 4.85125421\"\n",
      "[1] \"40 EOF ; RMS = 4.83489381\"\n",
      "[1] \"40 EOF ; RMS = 4.83627305\"\n",
      "[1] \"41 EOF ; RMS = 4.82081747\"\n",
      "[1] \"41 EOF ; RMS = 4.82272964\"\n",
      "[1] \"42 EOF ; RMS = 4.80743623\"\n",
      "[1] \"42 EOF ; RMS = 4.80858024\"\n",
      "[1] \"43 EOF ; RMS = 4.79600917\"\n",
      "[1] \"43 EOF ; RMS = 4.79836907\"\n",
      "[1] \"44 EOF ; RMS = 4.78760682\"\n",
      "[1] \"44 EOF ; RMS = 4.79021758\"\n",
      "[1] \"45 EOF ; RMS = 4.77183443\"\n",
      "[1] \"45 EOF ; RMS = 4.77261708\"\n",
      "[1] \"46 EOF ; RMS = 4.76288582\"\n",
      "[1] \"46 EOF ; RMS = 4.7654832\"\n",
      "[1] \"47 EOF ; RMS = 4.75794914\"\n",
      "[1] \"47 EOF ; RMS = 4.76113754\"\n",
      "[1] \"48 EOF ; RMS = 4.75481734\"\n",
      "[1] \"48 EOF ; RMS = 4.75882471\"\n",
      "[1] \"49 EOF ; RMS = 4.75248691\"\n",
      "[1] \"49 EOF ; RMS = 4.7564997\"\n",
      "[1] \"50 EOF ; RMS = 4.7483382\"\n",
      "[1] \"50 EOF ; RMS = 4.75095224\"\n",
      "[1] \"51 EOF ; RMS = 4.74123514\"\n",
      "[1] \"51 EOF ; RMS = 4.74239787\"\n",
      "[1] \"52 EOF ; RMS = 4.73317356\"\n",
      "[1] \"52 EOF ; RMS = 4.7354013\"\n",
      "[1] \"53 EOF ; RMS = 4.72987559\"\n",
      "[1] \"53 EOF ; RMS = 4.73430329\"\n",
      "[1] \"54 EOF ; RMS = 4.7336854\"\n",
      "[1] \"54 EOF ; RMS = 4.73882399\"\n",
      "[1] \"55 EOF ; RMS = 4.72961253\"\n",
      "[1] \"55 EOF ; RMS = 4.73137387\"\n",
      "[1] \"56 EOF ; RMS = 4.72267444\"\n",
      "[1] \"56 EOF ; RMS = 4.72524029\"\n",
      "[1] \"57 EOF ; RMS = 4.7271971\"\n"
     ]
    }
   ],
   "source": [
    "XRestruct=sinkr.dineof(u)\n",
    "YRestruct=sinkr.dineof(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f1b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "XRestruct_Fun=u\n",
    "YRestruct_Fun=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45a2de30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 5183)\n",
      "(605, 5183)\n"
     ]
    }
   ],
   "source": [
    "XRestruct_Fun=np.array(XRestruct[0])\n",
    "YRestruct_Fun=np.array(YRestruct[0])\n",
    "print(np.shape(XRestruct_Fun))\n",
    "print(np.shape(YRestruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81052a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xhat_train\n",
      "(485, 5183)\n",
      "Yhat_train\n",
      "(485, 5183)\n",
      "Xhat_val\n",
      "(120, 5183)\n",
      "Yhat_val\n",
      "(120, 5183)\n",
      "Xhat_test\n",
      "(120, 5183)\n",
      "Yhat_test\n",
      "(120, 5183)\n"
     ]
    }
   ],
   "source": [
    "Xhat=XRestruct_Fun\n",
    "Yhat=YRestruct_Fun\n",
    "Xhat_train = np.zeros([485,5183])\n",
    "Yhat_train = np.zeros([485,5183])\n",
    "Xhat_val = np.zeros([120,5183])\n",
    "Yhat_val = np.zeros([120,5183])\n",
    "Xhat_test = np.zeros([120,5183])\n",
    "Yhat_test = np.zeros([120,5183])\n",
    "\n",
    "for i in range (0,485):\n",
    "    for j in range (0,5183):    \n",
    "        Xhat_train[i][j] = Xhat[i][j]\n",
    "        Yhat_train[i][j] = Yhat[i][j]\n",
    "        \n",
    "for i in range (485,605):\n",
    "    for j in range (0,5183):\n",
    "        Xhat_val[i-485][j] = Xhat[i][j]\n",
    "        Yhat_val[i-485][j] = Yhat[i][j]     \n",
    "        \n",
    "for i in range (485,605):\n",
    "    for j in range (0,5183):\n",
    "        Xhat_test[i-485][j] = Xhat[i][j]\n",
    "        Yhat_test[i-485][j] = Yhat[i][j]\n",
    "print('Xhat_train')\n",
    "print(np.shape(Xhat_train))\n",
    "print('Yhat_train')\n",
    "print(np.shape(Yhat_train))\n",
    "print('Xhat_val')\n",
    "print(np.shape(Xhat_val))\n",
    "print('Yhat_val')\n",
    "print(np.shape(Yhat_val))\n",
    "print('Xhat_test')\n",
    "print(np.shape(Xhat_test))\n",
    "print('Yhat_test')\n",
    "print(np.shape(Yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f2887",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbe2ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(Xhat_train).float()\n",
    "y_train = torch.from_numpy(Yhat_train).float()\n",
    "x_val = torch.from_numpy(Xhat_val).float()\n",
    "y_val = torch.from_numpy(Yhat_val).float()\n",
    "x_test = torch.from_numpy(Xhat_test).float()\n",
    "y_test = torch.from_numpy(Yhat_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4542372a",
   "metadata": {},
   "source": [
    "## 會用到的函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67bddded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer_EncDec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.downConv = nn.Conv1d(in_channels=c_in,\n",
    "                                  out_channels=c_in,\n",
    "                                  kernel_size=3,\n",
    "                                  padding=2,\n",
    "                                  padding_mode='circular')\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask\n",
    "        )[0])\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask\n",
    "        )[0])\n",
    "\n",
    "        y = x = self.norm2(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm3(x + y)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "#masking\n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "\n",
    "class ProbMask():\n",
    "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
    "        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n",
    "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
    "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
    "                    torch.arange(H)[None, :, None],\n",
    "                    index, :].to(device)\n",
    "        self._mask = indicator.view(scores.shape).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "    \n",
    "# SelfAttention_Family\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "#from utils.masking import TriangularCausalMask, ProbMask\n",
    "import os\n",
    "\n",
    "\n",
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), A)\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "\n",
    "class ProbAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):  # n_top: c*ln(L_q)\n",
    "        # Q [B, H, L, D]\n",
    "        B, H, L_K, E = K.shape\n",
    "        _, _, L_Q, _ = Q.shape\n",
    "\n",
    "        # calculate the sampled Q_K\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k))  # real U = U_part(factor*ln(L_k))*L_q\n",
    "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\n",
    "\n",
    "        # find the Top_k query with sparisty measurement\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        # use the reduced Q to calculate Q_K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   M_top, :]  # factor*ln(L_q)\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))  # factor*ln(L_q)*L_k\n",
    "\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            # V_sum = V.sum(dim=-2)\n",
    "            V_sum = V.mean(dim=-2)\n",
    "            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
    "        else:  # use mask\n",
    "            assert (L_Q == L_V)  # requires that L_Q == L_V, i.e. for self-attention only\n",
    "            contex = V.cumsum(dim=-2)\n",
    "        return contex\n",
    "\n",
    "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
    "        B, H, L_V, D = V.shape\n",
    "\n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)  # nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "        torch.arange(H)[None, :, None],\n",
    "        index, :] = torch.matmul(attn, V).type_as(context_in)\n",
    "        if self.output_attention:\n",
    "            attns = (torch.ones([B, H, L_V, L_V]) / L_V).type_as(attn).to(attn.device)\n",
    "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
    "            return (context_in, attns)\n",
    "        else:\n",
    "            return (context_in, None)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "\n",
    "        queries = queries.transpose(2, 1)\n",
    "        keys = keys.transpose(2, 1)\n",
    "        values = values.transpose(2, 1)\n",
    "\n",
    "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()  # c*ln(L_k)\n",
    "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()  # c*ln(L_q)\n",
    "\n",
    "        U_part = U_part if U_part < L_K else L_K\n",
    "        u = u if u < L_Q else L_Q\n",
    "\n",
    "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n",
    "\n",
    "        # add scale factor\n",
    "        scale = self.scale or 1. / sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        # get the context\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        # update the context with selected top_k queries\n",
    "        context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n",
    "\n",
    "        return context.contiguous(), attn\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "    \n",
    "# Embed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DataEmbedding_wo_pos(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding_wo_pos, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class DataEmbedding_wo_pos_temp(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding_wo_pos_temp, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class DataEmbedding_wo_temp(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding_wo_temp, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd29497",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6490c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer\n",
    "class Transformer_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla Transformer with O(L^2) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.output_attention = configs.output_attention\n",
    "\n",
    "        # Embedding\n",
    "        if configs.embed_type == 0:\n",
    "            self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                            configs.dropout)\n",
    "            self.dec_embedding = DataEmbedding(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                           configs.dropout)\n",
    "        elif configs.embed_type == 1:\n",
    "            self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                    configs.dropout)\n",
    "            self.dec_embedding = DataEmbedding(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                    configs.dropout)\n",
    "        elif configs.embed_type == 2:\n",
    "            self.enc_embedding = DataEmbedding_wo_pos(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                    configs.dropout)\n",
    "            self.dec_embedding = DataEmbedding_wo_pos(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                    configs.dropout)\n",
    "\n",
    "        elif configs.embed_type == 3:\n",
    "            self.enc_embedding = DataEmbedding_wo_temp(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                    configs.dropout)\n",
    "            self.dec_embedding = DataEmbedding_wo_temp(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                    configs.dropout)\n",
    "        elif configs.embed_type == 4:\n",
    "            self.enc_embedding = DataEmbedding_wo_pos_temp(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                    configs.dropout)\n",
    "            self.dec_embedding = DataEmbedding_wo_pos_temp(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                    configs.dropout)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                      output_attention=configs.output_attention), configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation\n",
    "                ) for l in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(configs.d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(True, configs.factor, attention_dropout=configs.dropout, output_attention=False),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor, attention_dropout=configs.dropout, output_attention=False),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                )\n",
    "                for l in range(configs.d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(configs.d_model),\n",
    "            projection=nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc020a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdce2285",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2792178/3327222192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# optimize all cnn parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# the target label is not one-hotted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# input_shape = (-1,1,28,28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'configs'"
     ]
    }
   ],
   "source": [
    "model = Transformer_Model()\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.L1Loss()   # the target label is not one-hotted\n",
    "# input_shape = (-1,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49890d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss = nn.L1Loss()\n",
    "input_shape = (-1,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "536eb7bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parameters() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2792178/2613980333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformer_Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: parameters() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "list(Transformer_Model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a731f66a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "modules() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2792178/3182036823.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: modules() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "for module in model.modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c2423b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2792178/2829666696.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'configs'"
     ]
    }
   ],
   "source": [
    "model = Transformer_Model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728b0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694b971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38104017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f50f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f09ee64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c64f679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dlinear\n",
    "class DLinear_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    DLinear\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "\n",
    "        # Decompsition Kernel Size\n",
    "        kernel_size = 25\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.individual = configs.individual\n",
    "        self.channels = configs.enc_in\n",
    "\n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = nn.ModuleList()\n",
    "            self.Linear_Trend = nn.ModuleList()\n",
    "            self.Linear_Decoder = nn.ModuleList()\n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))\n",
    "                self.Linear_Seasonal[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))\n",
    "                self.Linear_Trend[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "                self.Linear_Decoder.append(nn.Linear(self.seq_len,self.pred_len))\n",
    "        else:\n",
    "            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)\n",
    "            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)\n",
    "            self.Linear_Decoder = nn.Linear(self.seq_len,self.pred_len)\n",
    "            self.Linear_Seasonal.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "            self.Linear_Trend.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)\n",
    "        if self.individual:\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)\n",
    "            for i in range(self.channels):\n",
    "                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])\n",
    "                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])\n",
    "        else:\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "\n",
    "        x = seasonal_output + trend_output\n",
    "        return x.permute(0,2,1) # to [Batch, Output length, Channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7c39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8bcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc65035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6509dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba4507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce6df4ad",
   "metadata": {},
   "source": [
    "# Pytorch 內建的 Transformer Model ? (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39dd3277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([485, 5183])\n",
      "torch.Size([485, 5183])\n",
      "torch.Size([120, 5183])\n",
      "torch.Size([120, 5183])\n",
      "torch.Size([120, 5183])\n",
      "torch.Size([120, 5183])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.from_numpy(Xhat_train).float()\n",
    "y_train = torch.from_numpy(Yhat_train).float()\n",
    "x_val = torch.from_numpy(Xhat_val).float()\n",
    "y_val = torch.from_numpy(Yhat_val).float()\n",
    "x_test = torch.from_numpy(Xhat_test).float()\n",
    "y_test = torch.from_numpy(Yhat_test).float()\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(x_val))\n",
    "print(np.shape(y_val))\n",
    "print(np.shape(x_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf56d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=32, num_encoder_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9711a79d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 485, 5183]' is invalid for input of size 2513755",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1563678/3781564224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m485\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5183\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m485\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5183\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 485, 5183]' is invalid for input of size 2513755"
     ]
    }
   ],
   "source": [
    "src = x_train.reshape(1,485,5183)\n",
    "tgt = y_train.reshape(,485,5183)\n",
    "print(np.shape(src))\n",
    "print(np.shape(tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d50f55b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "the feature number of src and tgt must be equal to d_model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1563678/1148311185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chou/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chou/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the feature number of src and tgt must be equal to d_model"
     ]
    }
   ],
   "source": [
    "out = transformer_model(src, tgt)\n",
    "print(np.shape(out))\n",
    "print(np.shape(out[0]))\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a988045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)\n",
    "print(np.shape(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fa9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chou",
   "language": "python",
   "name": "chou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
