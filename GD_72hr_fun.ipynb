{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdf66bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luhung3080/miniconda3/envs/chou/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch ver .  1.11.0+cu113\n",
      "Is CUDA available? True\n",
      "pynio ver .  1.5.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "print(\"pytorch ver . \",torch.__version__)\n",
    "print(\"Is CUDA available?\",torch.cuda.is_available())\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.utils.data as Data\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import xarray as xr\n",
    "import os\n",
    "os.environ['R_HOME'] = '/home/luhung3080/miniconda3/envs/chou/lib/R'\n",
    "from rpy2.robjects import r, numpy2ri\n",
    "numpy2ri.activate()\n",
    "from rpy2.robjects.packages import importr\n",
    "sinkr = importr('sinkr')\n",
    "import Nio\n",
    "print (\"pynio ver . \",Nio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcbe724a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>date</th>\n",
       "      <th>FCST_TIME</th>\n",
       "      <th>TAU</th>\n",
       "      <th>pm25_cal</th>\n",
       "      <th>pm25_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4.9510</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 10:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>4.4674</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 11:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.6159</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 12:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>3.9937</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPA001</td>\n",
       "      <td>2020-02-24 08:00:00</td>\n",
       "      <td>2020-02-24 13:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>3.9602</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092755</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 04:00:00</td>\n",
       "      <td>68</td>\n",
       "      <td>3.6190</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092756</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 05:00:00</td>\n",
       "      <td>69</td>\n",
       "      <td>3.7908</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092757</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 06:00:00</td>\n",
       "      <td>70</td>\n",
       "      <td>4.0454</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092758</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 07:00:00</td>\n",
       "      <td>71</td>\n",
       "      <td>3.9015</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092759</th>\n",
       "      <td>EPA080</td>\n",
       "      <td>2021-10-30 08:00:00</td>\n",
       "      <td>2021-11-02 08:00:00</td>\n",
       "      <td>72</td>\n",
       "      <td>2.7468</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3092760 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SITE_ID                 date            FCST_TIME  TAU  pm25_cal  \\\n",
       "0        EPA001  2020-02-24 08:00:00  2020-02-24 09:00:00    1    4.9510   \n",
       "1        EPA001  2020-02-24 08:00:00  2020-02-24 10:00:00    2    4.4674   \n",
       "2        EPA001  2020-02-24 08:00:00  2020-02-24 11:00:00    3    4.6159   \n",
       "3        EPA001  2020-02-24 08:00:00  2020-02-24 12:00:00    4    3.9937   \n",
       "4        EPA001  2020-02-24 08:00:00  2020-02-24 13:00:00    5    3.9602   \n",
       "...         ...                  ...                  ...  ...       ...   \n",
       "3092755  EPA080  2021-10-30 08:00:00  2021-11-02 04:00:00   68    3.6190   \n",
       "3092756  EPA080  2021-10-30 08:00:00  2021-11-02 05:00:00   69    3.7908   \n",
       "3092757  EPA080  2021-10-30 08:00:00  2021-11-02 06:00:00   70    4.0454   \n",
       "3092758  EPA080  2021-10-30 08:00:00  2021-11-02 07:00:00   71    3.9015   \n",
       "3092759  EPA080  2021-10-30 08:00:00  2021-11-02 08:00:00   72    2.7468   \n",
       "\n",
       "         pm25_obs  \n",
       "0            10.0  \n",
       "1            13.0  \n",
       "2            11.0  \n",
       "3            11.0  \n",
       "4             9.0  \n",
       "...           ...  \n",
       "3092755       4.0  \n",
       "3092756       7.0  \n",
       "3092757       7.0  \n",
       "3092758       4.0  \n",
       "3092759       4.0  \n",
       "\n",
       "[3092760 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('/home/luhung3080/Desktop/PycharmProjects/NCHUproject/Transformer/data_final.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be7cabcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 5112)\n",
      "(605, 5112)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#cal_PMf  \n",
    "###\n",
    "u=np.zeros([605,5112])\n",
    "for i in range (0,605):\n",
    "    a=np.array(data['pm25_cal'][5112*i:5112*i+5112])\n",
    "    u[i]=a.T\n",
    "\n",
    "###\n",
    "#obs_PMf\n",
    "###\n",
    "v=np.zeros([605,5112])\n",
    "for i in range (0,605):\n",
    "    a=np.array(data['pm25_obs'][5112*i:5112*i+5112])\n",
    "    v[i]=a.T\n",
    "\n",
    "print(np.shape(u))\n",
    "print(np.shape(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0387c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 5112)\n",
      "(605, 5112)\n"
     ]
    }
   ],
   "source": [
    "XRestruct_Fun=u\n",
    "YRestruct_Fun=v\n",
    "print(np.shape(XRestruct_Fun))\n",
    "print(np.shape(YRestruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46bbef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=XRestruct_Fun\n",
    "Yhat=YRestruct_Fun\n",
    "Xhat_train = np.zeros([544,5112])\n",
    "Yhat_train = np.zeros([544,5112])\n",
    "Xhat_test = np.zeros([61,5112])\n",
    "Yhat_test = np.zeros([61,5112])\n",
    "for i in range (0,544):\n",
    "    for j in range (0,5112):\n",
    "        Xhat_train[i][j] = Xhat[i][j]\n",
    "        Yhat_train[i][j] = Yhat[i][j]\n",
    "for i in range (544,605):\n",
    "    for j in range (0,5112):\n",
    "        Xhat_test[i-544][j] = Xhat[i][j]\n",
    "        Yhat_test[i-544][j] = Yhat[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0813db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xhat_train\n",
      "(544, 5112)\n",
      "Yhat_train\n",
      "(544, 5112)\n",
      "Xhat_test\n",
      "(61, 5112)\n",
      "Yhat_test\n",
      "(61, 5112)\n"
     ]
    }
   ],
   "source": [
    "print('Xhat_train')\n",
    "#print(Xhat_train)\n",
    "print(np.shape(Xhat_train))\n",
    "print('Yhat_train')\n",
    "#print(Yhat_train)\n",
    "print(np.shape(Yhat_train))\n",
    "print('Xhat_test')\n",
    "#print(Xhat_test)\n",
    "print(np.shape(Xhat_test))\n",
    "print('Yhat_test')\n",
    "#print(Yhat_test)\n",
    "print(np.shape(Yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa931c",
   "metadata": {},
   "source": [
    "# GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9bb27a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Xhat_train\n",
    "y = Yhat_train\n",
    "xt = Xhat_test\n",
    "yt = Yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e80b6c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (x,b0,b1):\n",
    "    y = b0 +  torch.mm(x , b1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9455a8c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 68584.6484, Testing loss 77295.8799\n",
      "Epoch 1, Training loss 68516.0469, Testing loss 77218.4953\n",
      "Epoch 2, Training loss 68447.4453, Testing loss 77141.1109\n",
      "Epoch 3, Training loss 68378.8516, Testing loss 77063.7223\n",
      "Epoch 4, Training loss 68310.2500, Testing loss 76986.3368\n",
      "Epoch 5, Training loss 68241.6484, Testing loss 76908.9519\n",
      "Epoch 6, Training loss 68173.0469, Testing loss 76831.5668\n",
      "Epoch 7, Training loss 68104.4453, Testing loss 76754.1837\n",
      "Epoch 8, Training loss 68035.8516, Testing loss 76676.7990\n",
      "Epoch 9, Training loss 67967.2578, Testing loss 76599.4155\n",
      "Epoch 10, Training loss 67898.6562, Testing loss 76522.0293\n",
      "Epoch 20, Training loss 67212.6719, Testing loss 75748.1743\n",
      "Epoch 30, Training loss 66526.6797, Testing loss 74974.3245\n",
      "Epoch 40, Training loss 65840.6953, Testing loss 74200.4737\n",
      "Epoch 50, Training loss 65154.6992, Testing loss 73426.6188\n",
      "Epoch 60, Training loss 64468.7070, Testing loss 72652.7662\n",
      "Epoch 70, Training loss 63782.7188, Testing loss 71878.9171\n",
      "Epoch 80, Training loss 63096.7422, Testing loss 71105.0658\n",
      "Epoch 90, Training loss 62410.7461, Testing loss 70331.2120\n",
      "Epoch 100, Training loss 61724.7617, Testing loss 69557.3610\n",
      "Epoch 110, Training loss 61038.7734, Testing loss 68783.5105\n",
      "Epoch 120, Training loss 60352.7852, Testing loss 68009.6591\n",
      "Epoch 130, Training loss 59666.8008, Testing loss 67235.8035\n",
      "Epoch 140, Training loss 58980.8047, Testing loss 66461.9537\n",
      "Epoch 150, Training loss 58294.8203, Testing loss 65688.1020\n",
      "Epoch 160, Training loss 57608.8320, Testing loss 64914.2503\n",
      "Epoch 170, Training loss 56922.8477, Testing loss 64140.3965\n",
      "Epoch 180, Training loss 56236.8516, Testing loss 63366.5465\n",
      "Epoch 190, Training loss 55550.8672, Testing loss 62592.6961\n",
      "Epoch 200, Training loss 54864.8789, Testing loss 61818.8433\n",
      "Epoch 210, Training loss 54178.8906, Testing loss 61044.9913\n",
      "Epoch 220, Training loss 53492.8984, Testing loss 60271.1377\n",
      "Epoch 230, Training loss 52806.9180, Testing loss 59497.2861\n",
      "Epoch 240, Training loss 52120.9258, Testing loss 58723.4340\n",
      "Epoch 250, Training loss 51434.9453, Testing loss 57949.5840\n",
      "Epoch 260, Training loss 50748.9492, Testing loss 57175.7320\n",
      "Epoch 270, Training loss 50062.9570, Testing loss 56401.8795\n",
      "Epoch 280, Training loss 49376.9727, Testing loss 55628.0268\n",
      "Epoch 290, Training loss 48690.9844, Testing loss 54854.1776\n",
      "Epoch 300, Training loss 48004.9922, Testing loss 54080.3222\n",
      "Epoch 310, Training loss 47319.0039, Testing loss 53306.4700\n",
      "Epoch 320, Training loss 46633.0195, Testing loss 52532.6210\n",
      "Epoch 330, Training loss 45947.0273, Testing loss 51758.7681\n",
      "Epoch 340, Training loss 45261.0430, Testing loss 50984.9163\n",
      "Epoch 350, Training loss 44575.0469, Testing loss 50211.0626\n",
      "Epoch 360, Training loss 43889.0664, Testing loss 49437.2137\n",
      "Epoch 370, Training loss 43203.0742, Testing loss 48663.3614\n",
      "Epoch 380, Training loss 42517.0859, Testing loss 47889.5071\n",
      "Epoch 390, Training loss 41831.0977, Testing loss 47115.6569\n",
      "Epoch 400, Training loss 41145.1133, Testing loss 46341.8046\n",
      "Epoch 410, Training loss 40459.1250, Testing loss 45567.9538\n",
      "Epoch 420, Training loss 39773.1328, Testing loss 44794.1001\n",
      "Epoch 430, Training loss 39087.1484, Testing loss 44020.2503\n",
      "Epoch 440, Training loss 38401.1562, Testing loss 43246.3976\n",
      "Epoch 450, Training loss 37715.1719, Testing loss 42472.5462\n",
      "Epoch 460, Training loss 37029.1797, Testing loss 41698.6943\n",
      "Epoch 470, Training loss 36343.1875, Testing loss 40924.8410\n",
      "Epoch 480, Training loss 35657.2031, Testing loss 40150.9906\n",
      "Epoch 490, Training loss 34971.2188, Testing loss 39377.1374\n",
      "Epoch 500, Training loss 34285.2266, Testing loss 38603.2872\n",
      "Epoch 510, Training loss 33599.2383, Testing loss 37829.4332\n",
      "Epoch 520, Training loss 32913.2500, Testing loss 37055.5832\n",
      "Epoch 530, Training loss 32227.2617, Testing loss 36281.7302\n",
      "Epoch 540, Training loss 31541.2754, Testing loss 35507.8800\n",
      "Epoch 550, Training loss 30855.2812, Testing loss 34734.0276\n",
      "Epoch 560, Training loss 30169.2988, Testing loss 33960.1748\n",
      "Epoch 570, Training loss 29483.3086, Testing loss 33186.3238\n",
      "Epoch 580, Training loss 28797.3223, Testing loss 32412.4714\n",
      "Epoch 590, Training loss 28111.3301, Testing loss 31638.6193\n",
      "Epoch 600, Training loss 27425.3438, Testing loss 30864.7667\n",
      "Epoch 610, Training loss 26739.3574, Testing loss 30090.9159\n",
      "Epoch 620, Training loss 26053.3672, Testing loss 29317.0635\n",
      "Epoch 630, Training loss 25367.3770, Testing loss 28543.2118\n",
      "Epoch 640, Training loss 24681.3906, Testing loss 27769.3603\n",
      "Epoch 650, Training loss 23995.4023, Testing loss 26995.5081\n",
      "Epoch 660, Training loss 23309.4121, Testing loss 26221.6572\n",
      "Epoch 670, Training loss 22623.4238, Testing loss 25447.8037\n",
      "Epoch 680, Training loss 21937.4355, Testing loss 24673.9523\n",
      "Epoch 690, Training loss 21251.4473, Testing loss 23900.1002\n",
      "Epoch 700, Training loss 20565.4609, Testing loss 23126.2489\n",
      "Epoch 710, Training loss 19879.4707, Testing loss 22352.3971\n",
      "Epoch 720, Training loss 19193.4844, Testing loss 21578.5454\n",
      "Epoch 730, Training loss 18507.4941, Testing loss 20804.6940\n",
      "Epoch 740, Training loss 17821.5078, Testing loss 20030.8416\n",
      "Epoch 750, Training loss 17135.5176, Testing loss 19256.9898\n",
      "Epoch 760, Training loss 16449.5254, Testing loss 18483.1322\n",
      "Epoch 770, Training loss 15763.5352, Testing loss 17709.2758\n",
      "Epoch 780, Training loss 15077.5410, Testing loss 16935.4192\n",
      "Epoch 790, Training loss 14391.5488, Testing loss 16161.5631\n",
      "Epoch 800, Training loss 13705.5557, Testing loss 15387.7069\n",
      "Epoch 810, Training loss 13019.5635, Testing loss 14613.8507\n",
      "Epoch 820, Training loss 12333.5713, Testing loss 13839.9937\n",
      "Epoch 830, Training loss 11647.5791, Testing loss 13066.1374\n",
      "Epoch 840, Training loss 10961.5859, Testing loss 12292.2810\n",
      "Epoch 850, Training loss 10275.5957, Testing loss 11518.4248\n",
      "Epoch 860, Training loss 9589.6035, Testing loss 10744.5679\n",
      "Epoch 870, Training loss 8903.6094, Testing loss 9970.7116\n",
      "Epoch 880, Training loss 8217.6182, Testing loss 9196.8554\n",
      "Epoch 890, Training loss 7531.6255, Testing loss 8422.9989\n",
      "Epoch 900, Training loss 6845.6333, Testing loss 7649.1424\n",
      "Epoch 910, Training loss 6159.6411, Testing loss 6875.2855\n",
      "Epoch 920, Training loss 5473.6484, Testing loss 6101.4298\n",
      "Epoch 930, Training loss 4787.6562, Testing loss 5327.5733\n",
      "Epoch 940, Training loss 4101.6641, Testing loss 4553.7170\n",
      "Epoch 950, Training loss 3415.6716, Testing loss 3779.8604\n",
      "Epoch 960, Training loss 2729.6792, Testing loss 3006.0040\n",
      "Epoch 970, Training loss 2043.6868, Testing loss 2232.1475\n",
      "Epoch 980, Training loss 1357.6945, Testing loss 1458.2910\n",
      "Epoch 990, Training loss 671.7021, Testing loss 684.4345\n",
      "Epoch 1000, Training loss 14.2810, Testing loss 73.9899\n",
      "Epoch 1010, Training loss 132.8953, Testing loss 117.0298\n",
      "Epoch 1020, Training loss 76.8746, Testing loss 76.9055\n",
      "Epoch 1030, Training loss 21.7411, Testing loss 11.3050\n",
      "Epoch 1040, Training loss 18.8855, Testing loss 10.7805\n",
      "Epoch 1050, Training loss 9.2677, Testing loss 10.0565\n",
      "Epoch 1060, Training loss 6.9663, Testing loss 8.1775\n",
      "Epoch 1070, Training loss 5.6842, Testing loss 5.9694\n",
      "Epoch 1080, Training loss 5.1427, Testing loss 5.9621\n",
      "Epoch 1090, Training loss 4.8710, Testing loss 5.6438\n",
      "Epoch 1100, Training loss 4.6822, Testing loss 5.6415\n",
      "Epoch 1110, Training loss 4.5375, Testing loss 5.6111\n",
      "Epoch 1120, Training loss 4.4164, Testing loss 5.6181\n",
      "Epoch 1130, Training loss 4.3134, Testing loss 5.6325\n",
      "Epoch 1140, Training loss 4.2261, Testing loss 5.6464\n",
      "Epoch 1150, Training loss 4.1482, Testing loss 5.6636\n",
      "Epoch 1160, Training loss 4.0804, Testing loss 5.6785\n",
      "Epoch 1170, Training loss 4.0205, Testing loss 5.6967\n",
      "Epoch 1180, Training loss 3.9605, Testing loss 5.7216\n",
      "Epoch 1190, Training loss 3.9120, Testing loss 5.7441\n",
      "Epoch 1200, Training loss 3.8673, Testing loss 5.7656\n",
      "Epoch 1210, Training loss 3.8295, Testing loss 5.7951\n",
      "Epoch 1220, Training loss 3.8058, Testing loss 5.8257\n",
      "Epoch 1230, Training loss 3.7776, Testing loss 5.8523\n",
      "Epoch 1240, Training loss 3.7567, Testing loss 5.8768\n",
      "Epoch 1250, Training loss 3.7374, Testing loss 5.9142\n",
      "Epoch 1260, Training loss 3.7216, Testing loss 5.9458\n",
      "Epoch 1270, Training loss 3.7172, Testing loss 5.9875\n",
      "Epoch 1280, Training loss 3.6972, Testing loss 6.0264\n",
      "Epoch 1290, Training loss 3.6991, Testing loss 6.0681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1300, Training loss 3.6943, Testing loss 6.0740\n",
      "Epoch 1310, Training loss 3.6817, Testing loss 6.1370\n",
      "Epoch 1320, Training loss 3.7046, Testing loss 6.1846\n",
      "Epoch 1330, Training loss 3.7204, Testing loss 6.2181\n",
      "Epoch 1340, Training loss 3.7074, Testing loss 6.2470\n",
      "Epoch 1350, Training loss 3.7137, Testing loss 6.2770\n",
      "Epoch 1360, Training loss 3.7443, Testing loss 6.3285\n",
      "Epoch 1370, Training loss 3.7485, Testing loss 6.3972\n",
      "Epoch 1380, Training loss 3.7589, Testing loss 6.4123\n",
      "Epoch 1390, Training loss 3.7960, Testing loss 6.4388\n",
      "Epoch 1400, Training loss 3.7942, Testing loss 6.5101\n",
      "Epoch 1410, Training loss 3.8190, Testing loss 6.5273\n",
      "Epoch 1420, Training loss 3.8464, Testing loss 6.5694\n",
      "Epoch 1430, Training loss 3.8665, Testing loss 6.6044\n",
      "Epoch 1440, Training loss 3.8719, Testing loss 6.6506\n",
      "Epoch 1450, Training loss 3.9355, Testing loss 6.6625\n",
      "Epoch 1460, Training loss 3.9189, Testing loss 6.7301\n",
      "Epoch 1470, Training loss 3.9437, Testing loss 6.7388\n",
      "Epoch 1480, Training loss 3.9579, Testing loss 6.7795\n",
      "Epoch 1490, Training loss 3.9796, Testing loss 6.8306\n",
      "Epoch 1500, Training loss 4.0223, Testing loss 6.8975\n",
      "Epoch 1510, Training loss 4.0547, Testing loss 6.9216\n",
      "Epoch 1520, Training loss 4.0407, Testing loss 6.9352\n",
      "Epoch 1530, Training loss 4.0845, Testing loss 7.0079\n",
      "Epoch 1540, Training loss 4.1200, Testing loss 7.0362\n",
      "Epoch 1550, Training loss 4.1484, Testing loss 7.0460\n",
      "Epoch 1560, Training loss 4.1870, Testing loss 7.1140\n",
      "Epoch 1570, Training loss 4.2009, Testing loss 7.1782\n",
      "Epoch 1580, Training loss 4.2102, Testing loss 7.1387\n",
      "Epoch 1590, Training loss 4.2205, Testing loss 7.2377\n",
      "Epoch 1600, Training loss 4.2251, Testing loss 7.2925\n",
      "Epoch 1610, Training loss 4.2737, Testing loss 7.2308\n",
      "Epoch 1620, Training loss 4.2570, Testing loss 7.2978\n",
      "Epoch 1630, Training loss 4.3041, Testing loss 7.3367\n",
      "Epoch 1640, Training loss 4.3431, Testing loss 7.3457\n",
      "Epoch 1650, Training loss 4.4154, Testing loss 7.3662\n",
      "Epoch 1660, Training loss 4.3465, Testing loss 7.4838\n",
      "Epoch 1670, Training loss 4.3805, Testing loss 7.3896\n",
      "Epoch 1680, Training loss 4.4061, Testing loss 7.4708\n",
      "Epoch 1690, Training loss 4.4355, Testing loss 7.4553\n",
      "Epoch 1700, Training loss 4.4598, Testing loss 7.5620\n",
      "Epoch 1710, Training loss 4.4721, Testing loss 7.5087\n",
      "Epoch 1720, Training loss 4.4844, Testing loss 7.6269\n",
      "Epoch 1730, Training loss 4.4962, Testing loss 7.5564\n",
      "Epoch 1740, Training loss 4.5078, Testing loss 7.6510\n",
      "Epoch 1750, Training loss 4.5037, Testing loss 7.5882\n",
      "Epoch 1760, Training loss 4.5061, Testing loss 7.6801\n",
      "Epoch 1770, Training loss 4.5321, Testing loss 7.6984\n",
      "Epoch 1780, Training loss 4.5883, Testing loss 7.6653\n",
      "Epoch 1790, Training loss 4.5221, Testing loss 7.6634\n",
      "Epoch 1800, Training loss 4.5991, Testing loss 7.8425\n",
      "Epoch 1810, Training loss 4.6094, Testing loss 7.7786\n",
      "Epoch 1820, Training loss 4.6444, Testing loss 7.8094\n",
      "Epoch 1830, Training loss 4.6767, Testing loss 7.8004\n",
      "Epoch 1840, Training loss 4.6472, Testing loss 7.8742\n",
      "Epoch 1850, Training loss 4.6626, Testing loss 7.8870\n",
      "Epoch 1860, Training loss 4.6594, Testing loss 7.8941\n",
      "Epoch 1870, Training loss 4.6261, Testing loss 7.8533\n",
      "Epoch 1880, Training loss 4.6309, Testing loss 7.8584\n",
      "Epoch 1890, Training loss 4.7030, Testing loss 7.9235\n",
      "Epoch 1900, Training loss 4.6782, Testing loss 7.9188\n",
      "Epoch 1910, Training loss 4.7141, Testing loss 7.9432\n",
      "Epoch 1920, Training loss 4.7346, Testing loss 7.9800\n",
      "Epoch 1930, Training loss 4.7393, Testing loss 7.9565\n",
      "Epoch 1940, Training loss 4.7353, Testing loss 7.9816\n",
      "Epoch 1950, Training loss 4.7095, Testing loss 7.9887\n",
      "Epoch 1960, Training loss 4.7534, Testing loss 8.0432\n",
      "Epoch 1970, Training loss 4.8444, Testing loss 8.0604\n",
      "Epoch 1980, Training loss 4.7258, Testing loss 8.0361\n",
      "Epoch 1990, Training loss 4.7460, Testing loss 8.0591\n",
      "Epoch 1991, Training loss 4.7664, Testing loss 8.0407\n",
      "Epoch 1992, Training loss 4.7166, Testing loss 8.0892\n",
      "Epoch 1993, Training loss 4.7451, Testing loss 8.1232\n",
      "Epoch 1994, Training loss 4.8149, Testing loss 8.0362\n",
      "Epoch 1995, Training loss 4.7440, Testing loss 8.0587\n",
      "Epoch 1996, Training loss 4.7495, Testing loss 8.1265\n",
      "Epoch 1997, Training loss 4.7945, Testing loss 8.1075\n",
      "Epoch 1998, Training loss 4.7982, Testing loss 8.0916\n",
      "Epoch 1999, Training loss 4.7699, Testing loss 8.0931\n",
      "Epoch 2000, Training loss 4.7761, Testing loss 8.1114\n"
     ]
    }
   ],
   "source": [
    "features = torch.from_numpy(x)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test = torch.from_numpy(xt)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "# beta0 = torch.randn(6816 , requires_grad = True)\n",
    "# beta1 = torch.randn([8520 , 6816], requires_grad = True)\n",
    "\n",
    "beta0 = torch.ones(5112 , requires_grad = True)\n",
    "beta1 = torch.ones([5112,5112], requires_grad = True)\n",
    "\n",
    "rate = 1e-3\n",
    "optimizer = optim.Adam([beta0 , beta1], lr=rate)\n",
    "\n",
    "epo = 2001\n",
    "loss = nn.L1Loss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "\n",
    "for epoch in range (epo):\n",
    "    yhats_train = model(features.float() , beta0 , beta1)\n",
    "    train_loss = loss(targets.float() , yhats_train)\n",
    "    train_error[epoch] = train_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward() \n",
    "    optimizer.step()    \n",
    "\n",
    "    yhats_test = model(x_test.float(), beta0, beta1) \n",
    "#     for i in range (25):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.nanmean(r)\n",
    "    # test_loss = loss(y_test , yhats_test)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Testing loss {test_loss.item():.4f}\")\n",
    "        # print('\\tBeta_0 : ' , beta0)\n",
    "        # print('\\tBeta_1 : ' , beta1)\n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                        f\" Testing loss {test_loss.item():.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f721e9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f60c1023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.600592600958763\n"
     ]
    }
   ],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc12ae1",
   "metadata": {},
   "source": [
    "# LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3022eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Xhat_train\n",
    "y = Yhat_train\n",
    "xt = Xhat_test\n",
    "yt = Yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "240e9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (x,b0,b1):\n",
    "    # y = b0 +  torch.matmul(x,b1)\n",
    "    y = b0 + torch.mm(x, b1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4d663c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 5436126208.0000, Testing loss 77373.2646\n",
      "Epoch 1, Training loss 47527829504.0000, Testing loss 232007.8190\n",
      "Epoch 2, Training loss 31795210240.0000, Testing loss 189804.9293\n",
      "Epoch 3, Training loss 21270343680.0000, Testing loss 155255.6386\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3221249/3182804866.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0myhats_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chou/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chou/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chou/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0mbe_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbe_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprev_flat_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = torch.from_numpy(x)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test = torch.from_numpy(xt)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "beta0 = torch.ones(5112 , requires_grad = True)\n",
    "beta1 = torch.ones([5112,5112] , requires_grad = True)\n",
    "\n",
    "\n",
    "rate = 1e-2\n",
    "optimizer = optim.LBFGS([beta0 , beta1] , lr = rate)\n",
    "\n",
    "epo = 201\n",
    "loss = nn.MSELoss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "\n",
    "for epoch in range (epo):  \n",
    "\n",
    "    def closure():\n",
    "        yhats_train = model(features.float() , beta0 , beta1)\n",
    "        train_loss = loss(targets.float() , yhats_train)\n",
    "        train_error[epoch] = train_loss\n",
    "        optimizer.zero_grad()\n",
    "        # if epoch == 0 :\n",
    "        #     train_loss.backward(retain_graph=True) \n",
    "        # else :\n",
    "        #     train_loss.backward()\n",
    "        train_loss.backward(retain_graph=True) \n",
    "        return train_loss\n",
    "    optimizer.step(closure)    \n",
    "\n",
    "    yhats_test = model(x_test.float(), beta0, beta1) \n",
    "#     for i in range (25):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.nanmean(r)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                    f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "        # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "        #             f\" Testing loss {test_loss:.4f}\")\n",
    "        # print('\\tBeta_0 : ' , beta0.grad)\n",
    "        # print('\\tBeta_1 : ' , beta1.grad)\n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                        f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "            # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "            #             f\" Testing loss {test_loss:.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dcc3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c50370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chou",
   "language": "python",
   "name": "chou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
